[2023-03-13T19:57:55.616+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:00:54+00:00 [queued]>
[2023-03-13T19:57:55.630+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:00:54+00:00 [queued]>
[2023-03-13T19:57:55.630+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2023-03-13T19:57:55.630+0000] {taskinstance.py:1280} INFO - Starting attempt 1 of 4
[2023-03-13T19:57:55.630+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2023-03-13T19:57:55.653+0000] {taskinstance.py:1300} INFO - Executing <Task(BashOperator): Chargement_des_données_in_ELK> on 2023-03-13 00:00:54+00:00
[2023-03-13T19:57:55.656+0000] {standard_task_runner.py:55} INFO - Started process 39660 to run task
[2023-03-13T19:57:55.661+0000] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'AUTOMATISATION', 'Chargement_des_données_in_ELK', 'scheduled__2023-03-13T00:00:54+00:00', '--job-id', '147', '--raw', '--subdir', 'DAGS_FOLDER/main.py', '--cfg-path', '/tmp/tmphrzgavmm']
[2023-03-13T19:57:55.663+0000] {standard_task_runner.py:83} INFO - Job 147: Subtask Chargement_des_données_in_ELK
[2023-03-13T19:57:55.730+0000] {task_command.py:388} INFO - Running <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:00:54+00:00 [running]> on host dev
[2023-03-13T19:57:55.803+0000] {taskinstance.py:1507} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=AUTOMATISATION
AIRFLOW_CTX_TASK_ID=Chargement_des_données_in_ELK
AIRFLOW_CTX_EXECUTION_DATE=2023-03-13T00:00:54+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-13T00:00:54+00:00
[2023-03-13T19:57:55.805+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-03-13T19:57:55.806+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'bin/logstash --path.config /etc/logstash/conf.d/data.conf']
[2023-03-13T19:57:55.816+0000] {subprocess.py:86} INFO - Output:
[2023-03-13T19:57:55.818+0000] {subprocess.py:93} INFO - /usr/bin/bash: ligne 1: bin/logstash: Aucun fichier ou dossier de ce type
[2023-03-13T19:57:55.818+0000] {subprocess.py:97} INFO - Command exited with return code 127
[2023-03-13T19:57:55.829+0000] {taskinstance.py:1768} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/dev/airflow/airflow/lib/python3.10/site-packages/airflow/operators/bash.py", line 196, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 127.
[2023-03-13T19:57:55.833+0000] {taskinstance.py:1318} INFO - Marking task as UP_FOR_RETRY. dag_id=AUTOMATISATION, task_id=Chargement_des_données_in_ELK, execution_date=20230313T000054, start_date=20230313T195755, end_date=20230313T195755
[2023-03-13T19:57:55.853+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 147 for task Chargement_des_données_in_ELK (Bash command failed. The command returned a non-zero exit code 127.; 39660)
[2023-03-13T19:57:55.872+0000] {local_task_job.py:208} INFO - Task exited with return code 1
[2023-03-13T19:57:55.892+0000] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-03-13T20:48:00.038+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:00:54+00:00 [queued]>
[2023-03-13T20:48:00.053+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:00:54+00:00 [queued]>
[2023-03-13T20:48:00.053+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2023-03-13T20:48:00.054+0000] {taskinstance.py:1280} INFO - Starting attempt 1 of 4
[2023-03-13T20:48:00.054+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2023-03-13T20:48:00.075+0000] {taskinstance.py:1300} INFO - Executing <Task(BashOperator): Chargement_des_données_in_ELK> on 2023-03-13 00:00:54+00:00
[2023-03-13T20:48:00.079+0000] {standard_task_runner.py:55} INFO - Started process 50493 to run task
[2023-03-13T20:48:00.083+0000] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'AUTOMATISATION', 'Chargement_des_données_in_ELK', 'scheduled__2023-03-13T00:00:54+00:00', '--job-id', '315', '--raw', '--subdir', 'DAGS_FOLDER/main.py', '--cfg-path', '/tmp/tmp5_yqlo_4']
[2023-03-13T20:48:00.085+0000] {standard_task_runner.py:83} INFO - Job 315: Subtask Chargement_des_données_in_ELK
[2023-03-13T20:48:00.151+0000] {task_command.py:388} INFO - Running <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:00:54+00:00 [running]> on host dev
[2023-03-13T20:48:00.228+0000] {taskinstance.py:1507} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=AUTOMATISATION
AIRFLOW_CTX_TASK_ID=Chargement_des_données_in_ELK
AIRFLOW_CTX_EXECUTION_DATE=2023-03-13T00:00:54+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-13T00:00:54+00:00
[2023-03-13T20:48:00.230+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-03-13T20:48:00.231+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'echo dev | sudo -S /usr/share/logstash/bin/logstash --path.settings=/etc/logstash/ -f /etc/logstash/conf.d/data.conf']
[2023-03-13T20:48:00.241+0000] {subprocess.py:86} INFO - Output:
[2023-03-13T20:48:00.339+0000] {subprocess.py:93} INFO - [sudo] Mot de passe de dev : Using bundled JDK: /usr/share/logstash/jdk
[2023-03-13T20:48:00.634+0000] {subprocess.py:93} INFO - OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
[2023-03-13T20:48:28.147+0000] {subprocess.py:93} INFO - Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties
[2023-03-13T20:48:28.282+0000] {subprocess.py:93} INFO - [2023-03-13T20:48:28,279][INFO ][logstash.runner          ] Log4j configuration path used is: /etc/logstash/log4j2.properties
[2023-03-13T20:48:28.293+0000] {subprocess.py:93} INFO - [2023-03-13T20:48:28,293][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.15.2", "jruby.version"=>"jruby 9.2.19.0 (2.5.8) 2021-06-15 55810c552b OpenJDK 64-Bit Server VM 11.0.12+7 on 11.0.12+7 +indy +jit [linux-x86_64]"}
[2023-03-13T20:48:28.918+0000] {subprocess.py:93} INFO - [2023-03-13T20:48:28,918][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2023-03-13T20:48:30.904+0000] {subprocess.py:93} INFO - [2023-03-13T20:48:30,901][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2023-03-13T20:48:31.770+0000] {subprocess.py:93} INFO - [2023-03-13T20:48:31,769][INFO ][org.reflections.Reflections] Reflections took 105 ms to scan 1 urls, producing 120 keys and 417 values
[2023-03-13T20:48:33.359+0000] {subprocess.py:93} INFO - [2023-03-13T20:48:33,358][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2023-03-13T20:48:33.867+0000] {subprocess.py:93} INFO - [2023-03-13T20:48:33,864][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2023-03-13T20:48:34.076+0000] {subprocess.py:93} INFO - [2023-03-13T20:48:34,075][WARN ][logstash.outputs.elasticsearch][main] Restored connection to ES instance {:url=>"http://localhost:9200/"}
[2023-03-13T20:48:34.135+0000] {subprocess.py:93} INFO - [2023-03-13T20:48:34,134][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch version determined (7.15.2) {:es_version=>7}
[2023-03-13T20:48:34.139+0000] {subprocess.py:93} INFO - [2023-03-13T20:48:34,138][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7}
[2023-03-13T20:48:34.375+0000] {subprocess.py:93} INFO - [2023-03-13T20:48:34,374][INFO ][logstash.outputs.elasticsearch][main] Using a default mapping template {:es_version=>7, :ecs_compatibility=>:disabled}
[2023-03-13T20:48:34.440+0000] {subprocess.py:93} INFO - [2023-03-13T20:48:34,439][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1000, "pipeline.sources"=>["/etc/logstash/conf.d/data.conf"], :thread=>"#<Thread:0x34a9ef35 run>"}
[2023-03-13T20:48:35.643+0000] {subprocess.py:93} INFO - [2023-03-13T20:48:35,642][INFO ][logstash.javapipeline    ][main] Pipeline Java execution initialization time {"seconds"=>1.2}
[2023-03-13T20:48:35.781+0000] {subprocess.py:93} INFO - [2023-03-13T20:48:35,780][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2023-03-13T20:48:35.896+0000] {subprocess.py:93} INFO - [2023-03-13T20:48:35,895][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2023-03-13T20:48:37.868+0000] {subprocess.py:93} INFO - [2023-03-13T20:48:37,867][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.029161s) SELECT CAST(current_setting('server_version_num') AS integer) AS v
[2023-03-13T20:48:38.097+0000] {subprocess.py:93} INFO - [2023-03-13T20:48:38,097][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.056575s) SELECT count(*) AS "count" FROM (SELECT * FROM customers) AS "t1" LIMIT 1
[2023-03-13T20:48:38.552+0000] {subprocess.py:93} INFO - [2023-03-13T20:48:38,552][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.422839s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 0
[2023-03-13T20:48:47.617+0000] {subprocess.py:93} INFO - [2023-03-13T20:48:47,617][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.357736s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 50000
[2023-03-13T20:48:52.855+0000] {subprocess.py:93} INFO - [2023-03-13T20:48:52,855][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.206964s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 100000
[2023-03-13T20:48:55.789+0000] {subprocess.py:93} INFO - [2023-03-13T20:48:55,788][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.127220s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 150000
[2023-03-13T20:48:58.414+0000] {subprocess.py:93} INFO - [2023-03-13T20:48:58,414][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.159058s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 200000
[2023-03-13T20:49:00.971+0000] {subprocess.py:93} INFO - [2023-03-13T20:49:00,970][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.108402s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 250000
[2023-03-13T20:49:03.928+0000] {subprocess.py:93} INFO - [2023-03-13T20:49:03,928][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.146469s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 300000
[2023-03-13T20:49:06.585+0000] {subprocess.py:93} INFO - [2023-03-13T20:49:06,585][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.122509s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 350000
[2023-03-13T20:49:09.150+0000] {subprocess.py:93} INFO - [2023-03-13T20:49:09,149][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.145359s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 400000
[2023-03-13T20:49:12.233+0000] {subprocess.py:93} INFO - [2023-03-13T20:49:12,233][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.197824s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 450000
[2023-03-13T20:49:14.784+0000] {subprocess.py:93} INFO - [2023-03-13T20:49:14,784][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.139936s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 500000
[2023-03-13T20:49:17.204+0000] {subprocess.py:93} INFO - [2023-03-13T20:49:17,204][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.134780s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 550000
[2023-03-13T20:49:29.419+0000] {subprocess.py:93} INFO - [2023-03-13T20:49:29,419][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.140881s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 600000
[2023-03-13T20:49:31.772+0000] {subprocess.py:93} INFO - [2023-03-13T20:49:31,772][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.150522s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 650000
[2023-03-13T20:49:34.267+0000] {subprocess.py:93} INFO - [2023-03-13T20:49:34,266][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.116866s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 700000
[2023-03-13T20:49:36.465+0000] {subprocess.py:93} INFO - [2023-03-13T20:49:36,465][INFO ][logstash.javapipeline    ][main] Pipeline terminated {"pipeline.id"=>"main"}
[2023-03-13T20:49:36.618+0000] {subprocess.py:93} INFO - [2023-03-13T20:49:36,617][INFO ][logstash.pipelinesregistry] Removed pipeline from registry successfully {:pipeline_id=>:main}
[2023-03-13T20:49:36.669+0000] {subprocess.py:93} INFO - [2023-03-13T20:49:36,669][INFO ][logstash.runner          ] Logstash shut down.
[2023-03-13T20:49:36.837+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-03-13T20:49:36.866+0000] {taskinstance.py:1318} INFO - Marking task as SUCCESS. dag_id=AUTOMATISATION, task_id=Chargement_des_données_in_ELK, execution_date=20230313T000054, start_date=20230313T204800, end_date=20230313T204936
[2023-03-13T20:49:36.903+0000] {local_task_job.py:208} INFO - Task exited with return code 0
[2023-03-13T20:49:36.917+0000] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-03-14T09:41:13.092+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:00:54+00:00 [queued]>
[2023-03-14T09:41:13.105+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:00:54+00:00 [queued]>
[2023-03-14T09:41:13.106+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2023-03-14T09:41:13.106+0000] {taskinstance.py:1280} INFO - Starting attempt 1 of 4
[2023-03-14T09:41:13.106+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2023-03-14T09:41:13.126+0000] {taskinstance.py:1300} INFO - Executing <Task(BashOperator): Chargement_des_données_in_ELK> on 2023-03-13 00:00:54+00:00
[2023-03-14T09:41:13.129+0000] {standard_task_runner.py:55} INFO - Started process 28025 to run task
[2023-03-14T09:41:13.134+0000] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'AUTOMATISATION', 'Chargement_des_données_in_ELK', 'scheduled__2023-03-13T00:00:54+00:00', '--job-id', '559', '--raw', '--subdir', 'DAGS_FOLDER/main.py', '--cfg-path', '/tmp/tmpnu9aq9zi']
[2023-03-14T09:41:13.136+0000] {standard_task_runner.py:83} INFO - Job 559: Subtask Chargement_des_données_in_ELK
[2023-03-14T09:41:13.206+0000] {task_command.py:388} INFO - Running <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:00:54+00:00 [running]> on host dev
[2023-03-14T09:41:13.277+0000] {taskinstance.py:1507} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=AUTOMATISATION
AIRFLOW_CTX_TASK_ID=Chargement_des_données_in_ELK
AIRFLOW_CTX_EXECUTION_DATE=2023-03-13T00:00:54+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-13T00:00:54+00:00
[2023-03-14T09:41:13.279+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-03-14T09:41:13.279+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'echo dev | sudo -S /usr/share/logstash/bin/logstash --path.settings=/etc/logstash/ -f /etc/logstash/conf.d/data.conf']
[2023-03-14T09:41:13.288+0000] {subprocess.py:86} INFO - Output:
[2023-03-14T09:41:13.383+0000] {subprocess.py:93} INFO - [sudo] Mot de passe de dev : Using bundled JDK: /usr/share/logstash/jdk
[2023-03-14T09:41:13.655+0000] {subprocess.py:93} INFO - OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
[2023-03-14T09:41:37.841+0000] {subprocess.py:93} INFO - Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties
[2023-03-14T09:41:37.986+0000] {subprocess.py:93} INFO - [2023-03-14T09:41:37,982][INFO ][logstash.runner          ] Log4j configuration path used is: /etc/logstash/log4j2.properties
[2023-03-14T09:41:37.998+0000] {subprocess.py:93} INFO - [2023-03-14T09:41:37,997][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.15.2", "jruby.version"=>"jruby 9.2.19.0 (2.5.8) 2021-06-15 55810c552b OpenJDK 64-Bit Server VM 11.0.12+7 on 11.0.12+7 +indy +jit [linux-x86_64]"}
[2023-03-14T09:41:38.517+0000] {subprocess.py:93} INFO - [2023-03-14T09:41:38,516][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2023-03-14T09:41:40.451+0000] {subprocess.py:93} INFO - [2023-03-14T09:41:40,450][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2023-03-14T09:41:41.224+0000] {subprocess.py:93} INFO - [2023-03-14T09:41:41,223][INFO ][org.reflections.Reflections] Reflections took 109 ms to scan 1 urls, producing 120 keys and 417 values
[2023-03-14T09:41:42.739+0000] {subprocess.py:93} INFO - [2023-03-14T09:41:42,738][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2023-03-14T09:41:43.174+0000] {subprocess.py:93} INFO - [2023-03-14T09:41:43,172][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2023-03-14T09:41:43.374+0000] {subprocess.py:93} INFO - [2023-03-14T09:41:43,374][WARN ][logstash.outputs.elasticsearch][main] Restored connection to ES instance {:url=>"http://localhost:9200/"}
[2023-03-14T09:41:43.440+0000] {subprocess.py:93} INFO - [2023-03-14T09:41:43,439][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch version determined (7.15.2) {:es_version=>7}
[2023-03-14T09:41:43.443+0000] {subprocess.py:93} INFO - [2023-03-14T09:41:43,443][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7}
[2023-03-14T09:41:43.656+0000] {subprocess.py:93} INFO - [2023-03-14T09:41:43,655][INFO ][logstash.outputs.elasticsearch][main] Using a default mapping template {:es_version=>7, :ecs_compatibility=>:disabled}
[2023-03-14T09:41:43.710+0000] {subprocess.py:93} INFO - [2023-03-14T09:41:43,709][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1000, "pipeline.sources"=>["/etc/logstash/conf.d/data.conf"], :thread=>"#<Thread:0x26ff8d8 run>"}
[2023-03-14T09:41:44.919+0000] {subprocess.py:93} INFO - [2023-03-14T09:41:44,918][INFO ][logstash.javapipeline    ][main] Pipeline Java execution initialization time {"seconds"=>1.2}
[2023-03-14T09:41:45.036+0000] {subprocess.py:93} INFO - [2023-03-14T09:41:45,035][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2023-03-14T09:41:45.157+0000] {subprocess.py:93} INFO - [2023-03-14T09:41:45,157][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2023-03-14T09:41:46.656+0000] {subprocess.py:93} INFO - [2023-03-14T09:41:46,655][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.026697s) SELECT CAST(current_setting('server_version_num') AS integer) AS v
[2023-03-14T09:41:46.916+0000] {subprocess.py:93} INFO - [2023-03-14T09:41:46,916][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.061951s) SELECT count(*) AS "count" FROM (SELECT * FROM customers) AS "t1" LIMIT 1
[2023-03-14T09:41:47.426+0000] {subprocess.py:93} INFO - [2023-03-14T09:41:47,425][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.479912s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 0
[2023-03-14T09:41:54.928+0000] {subprocess.py:93} INFO - [2023-03-14T09:41:54,927][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.277792s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 50000
[2023-03-14T09:41:59.898+0000] {subprocess.py:93} INFO - [2023-03-14T09:41:59,898][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.097485s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 100000
[2023-03-14T09:42:02.884+0000] {subprocess.py:93} INFO - [2023-03-14T09:42:02,884][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.126288s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 150000
[2023-03-14T09:42:05.777+0000] {subprocess.py:93} INFO - [2023-03-14T09:42:05,777][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.130520s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 200000
[2023-03-14T09:42:08.554+0000] {subprocess.py:93} INFO - [2023-03-14T09:42:08,554][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.112214s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 250000
[2023-03-14T09:42:11.112+0000] {subprocess.py:93} INFO - [2023-03-14T09:42:11,111][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.125404s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 300000
[2023-03-14T09:42:13.470+0000] {subprocess.py:93} INFO - [2023-03-14T09:42:13,470][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.120506s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 350000
[2023-03-14T09:42:15.828+0000] {subprocess.py:93} INFO - [2023-03-14T09:42:15,827][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.121346s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 400000
[2023-03-14T09:42:18.331+0000] {subprocess.py:93} INFO - [2023-03-14T09:42:18,331][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.128081s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 450000
[2023-03-14T09:42:20.696+0000] {subprocess.py:93} INFO - [2023-03-14T09:42:20,695][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.123274s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 500000
[2023-03-14T09:42:23.021+0000] {subprocess.py:93} INFO - [2023-03-14T09:42:23,020][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.123392s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 550000
[2023-03-14T09:42:25.306+0000] {subprocess.py:93} INFO - [2023-03-14T09:42:25,305][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.136138s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 600000
[2023-03-14T09:42:27.539+0000] {subprocess.py:93} INFO - [2023-03-14T09:42:27,539][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.136488s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 650000
[2023-03-14T09:42:30.710+0000] {subprocess.py:93} INFO - [2023-03-14T09:42:30,710][INFO ][logstash.javapipeline    ][main] Pipeline terminated {"pipeline.id"=>"main"}
[2023-03-14T09:42:30.880+0000] {subprocess.py:93} INFO - [2023-03-14T09:42:30,879][INFO ][logstash.pipelinesregistry] Removed pipeline from registry successfully {:pipeline_id=>:main}
[2023-03-14T09:42:30.930+0000] {subprocess.py:93} INFO - [2023-03-14T09:42:30,930][INFO ][logstash.runner          ] Logstash shut down.
[2023-03-14T09:42:31.106+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-03-14T09:42:31.135+0000] {taskinstance.py:1318} INFO - Marking task as SUCCESS. dag_id=AUTOMATISATION, task_id=Chargement_des_données_in_ELK, execution_date=20230313T000054, start_date=20230314T094113, end_date=20230314T094231
[2023-03-14T09:42:31.192+0000] {local_task_job.py:208} INFO - Task exited with return code 0
[2023-03-14T09:42:31.208+0000] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-03-15T07:13:07.218+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:00:54+00:00 [queued]>
[2023-03-15T07:13:07.224+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:00:54+00:00 [queued]>
[2023-03-15T07:13:07.224+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2023-03-15T07:13:07.224+0000] {taskinstance.py:1280} INFO - Starting attempt 1 of 4
[2023-03-15T07:13:07.224+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2023-03-15T07:13:07.238+0000] {taskinstance.py:1300} INFO - Executing <Task(BashOperator): Chargement_des_données_in_ELK> on 2023-03-13 00:00:54+00:00
[2023-03-15T07:13:07.240+0000] {standard_task_runner.py:55} INFO - Started process 119388 to run task
[2023-03-15T07:13:07.241+0000] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'AUTOMATISATION', 'Chargement_des_données_in_ELK', 'scheduled__2023-03-13T00:00:54+00:00', '--job-id', '682', '--raw', '--subdir', 'DAGS_FOLDER/main.py', '--cfg-path', '/tmp/tmpsdq1yr5q']
[2023-03-15T07:13:07.242+0000] {standard_task_runner.py:83} INFO - Job 682: Subtask Chargement_des_données_in_ELK
[2023-03-15T07:13:07.273+0000] {task_command.py:388} INFO - Running <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:00:54+00:00 [running]> on host dev
[2023-03-15T07:13:07.310+0000] {taskinstance.py:1507} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=AUTOMATISATION
AIRFLOW_CTX_TASK_ID=Chargement_des_données_in_ELK
AIRFLOW_CTX_EXECUTION_DATE=2023-03-13T00:00:54+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-13T00:00:54+00:00
[2023-03-15T07:13:07.311+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-03-15T07:13:07.311+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'echo dev | sudo -S /usr/share/logstash/bin/logstash --path.settings=/etc/logstash/ -f /etc/logstash/conf.d/data.conf']
[2023-03-15T07:13:07.315+0000] {subprocess.py:86} INFO - Output:
[2023-03-15T07:13:07.351+0000] {subprocess.py:93} INFO - [sudo] Mot de passe de dev : Using bundled JDK: /usr/share/logstash/jdk
[2023-03-15T07:13:07.471+0000] {subprocess.py:93} INFO - OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
[2023-03-15T07:13:20.535+0000] {subprocess.py:93} INFO - Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties
[2023-03-15T07:13:20.586+0000] {subprocess.py:93} INFO - [2023-03-15T07:13:20,585][INFO ][logstash.runner          ] Log4j configuration path used is: /etc/logstash/log4j2.properties
[2023-03-15T07:13:20.591+0000] {subprocess.py:93} INFO - [2023-03-15T07:13:20,591][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.15.2", "jruby.version"=>"jruby 9.2.19.0 (2.5.8) 2021-06-15 55810c552b OpenJDK 64-Bit Server VM 11.0.12+7 on 11.0.12+7 +indy +jit [linux-x86_64]"}
[2023-03-15T07:13:20.804+0000] {subprocess.py:93} INFO - [2023-03-15T07:13:20,804][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2023-03-15T07:13:21.804+0000] {subprocess.py:93} INFO - [2023-03-15T07:13:21,804][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2023-03-15T07:13:22.178+0000] {subprocess.py:93} INFO - [2023-03-15T07:13:22,178][INFO ][org.reflections.Reflections] Reflections took 41 ms to scan 1 urls, producing 120 keys and 417 values
[2023-03-15T07:13:22.857+0000] {subprocess.py:93} INFO - [2023-03-15T07:13:22,856][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2023-03-15T07:13:23.118+0000] {subprocess.py:93} INFO - [2023-03-15T07:13:23,116][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2023-03-15T07:13:23.214+0000] {subprocess.py:93} INFO - [2023-03-15T07:13:23,213][WARN ][logstash.outputs.elasticsearch][main] Restored connection to ES instance {:url=>"http://localhost:9200/"}
[2023-03-15T07:13:23.240+0000] {subprocess.py:93} INFO - [2023-03-15T07:13:23,239][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch version determined (7.15.2) {:es_version=>7}
[2023-03-15T07:13:23.241+0000] {subprocess.py:93} INFO - [2023-03-15T07:13:23,241][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7}
[2023-03-15T07:13:23.345+0000] {subprocess.py:93} INFO - [2023-03-15T07:13:23,344][INFO ][logstash.outputs.elasticsearch][main] Using a default mapping template {:es_version=>7, :ecs_compatibility=>:disabled}
[2023-03-15T07:13:23.367+0000] {subprocess.py:93} INFO - [2023-03-15T07:13:23,366][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1000, "pipeline.sources"=>["/etc/logstash/conf.d/data.conf"], :thread=>"#<Thread:0xde0b891 run>"}
[2023-03-15T07:13:23.868+0000] {subprocess.py:93} INFO - [2023-03-15T07:13:23,868][INFO ][logstash.javapipeline    ][main] Pipeline Java execution initialization time {"seconds"=>0.5}
[2023-03-15T07:13:23.936+0000] {subprocess.py:93} INFO - [2023-03-15T07:13:23,935][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2023-03-15T07:13:24.004+0000] {subprocess.py:93} INFO - [2023-03-15T07:13:23,987][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2023-03-15T07:13:24.802+0000] {subprocess.py:93} INFO - [2023-03-15T07:13:24,802][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.012651s) SELECT CAST(current_setting('server_version_num') AS integer) AS v
[2023-03-15T07:13:24.889+0000] {subprocess.py:93} INFO - [2023-03-15T07:13:24,888][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.002753s) SELECT count(*) AS "count" FROM (SELECT * FROM customers) AS "t1" LIMIT 1
[2023-03-15T07:13:24.985+0000] {subprocess.py:93} INFO - [2023-03-15T07:13:24,985][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.081382s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 0
[2023-03-15T07:13:27.289+0000] {subprocess.py:93} INFO - [2023-03-15T07:13:27,289][INFO ][logstash.javapipeline    ][main] Pipeline terminated {"pipeline.id"=>"main"}
[2023-03-15T07:13:27.544+0000] {subprocess.py:93} INFO - [2023-03-15T07:13:27,544][INFO ][logstash.pipelinesregistry] Removed pipeline from registry successfully {:pipeline_id=>:main}
[2023-03-15T07:13:27.566+0000] {subprocess.py:93} INFO - [2023-03-15T07:13:27,566][INFO ][logstash.runner          ] Logstash shut down.
[2023-03-15T07:13:27.631+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-03-15T07:13:27.647+0000] {taskinstance.py:1318} INFO - Marking task as SUCCESS. dag_id=AUTOMATISATION, task_id=Chargement_des_données_in_ELK, execution_date=20230313T000054, start_date=20230315T071307, end_date=20230315T071327
[2023-03-15T07:13:27.694+0000] {local_task_job.py:208} INFO - Task exited with return code 0
[2023-03-15T07:13:27.706+0000] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
