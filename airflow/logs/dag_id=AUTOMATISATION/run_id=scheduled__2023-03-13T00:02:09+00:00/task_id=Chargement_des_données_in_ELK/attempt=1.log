[2023-03-13T21:40:19.797+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:02:09+00:00 [queued]>
[2023-03-13T21:40:19.809+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:02:09+00:00 [queued]>
[2023-03-13T21:40:19.810+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2023-03-13T21:40:19.810+0000] {taskinstance.py:1280} INFO - Starting attempt 1 of 4
[2023-03-13T21:40:19.810+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2023-03-13T21:40:19.842+0000] {taskinstance.py:1300} INFO - Executing <Task(BashOperator): Chargement_des_données_in_ELK> on 2023-03-13 00:02:09+00:00
[2023-03-13T21:40:19.846+0000] {standard_task_runner.py:55} INFO - Started process 62438 to run task
[2023-03-13T21:40:19.851+0000] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'AUTOMATISATION', 'Chargement_des_données_in_ELK', 'scheduled__2023-03-13T00:02:09+00:00', '--job-id', '365', '--raw', '--subdir', 'DAGS_FOLDER/main.py', '--cfg-path', '/tmp/tmp5evbi3ye']
[2023-03-13T21:40:19.853+0000] {standard_task_runner.py:83} INFO - Job 365: Subtask Chargement_des_données_in_ELK
[2023-03-13T21:40:19.929+0000] {task_command.py:388} INFO - Running <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:02:09+00:00 [running]> on host dev
[2023-03-13T21:40:20.032+0000] {taskinstance.py:1507} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=AUTOMATISATION
AIRFLOW_CTX_TASK_ID=Chargement_des_données_in_ELK
AIRFLOW_CTX_EXECUTION_DATE=2023-03-13T00:02:09+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-13T00:02:09+00:00
[2023-03-13T21:40:20.034+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-03-13T21:40:20.035+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'echo dev | sudo -S /usr/share/logstash/bin/logstash --path.settings=/etc/logstash/ -f /etc/logstash/conf.d/data.conf']
[2023-03-13T21:40:20.044+0000] {subprocess.py:86} INFO - Output:
[2023-03-13T21:40:20.141+0000] {subprocess.py:93} INFO - [sudo] Mot de passe de dev : Using bundled JDK: /usr/share/logstash/jdk
[2023-03-13T21:40:20.446+0000] {subprocess.py:93} INFO - OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
[2023-03-13T21:40:46.605+0000] {subprocess.py:93} INFO - Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties
[2023-03-13T21:40:46.735+0000] {subprocess.py:93} INFO - [2023-03-13T21:40:46,732][INFO ][logstash.runner          ] Log4j configuration path used is: /etc/logstash/log4j2.properties
[2023-03-13T21:40:46.747+0000] {subprocess.py:93} INFO - [2023-03-13T21:40:46,747][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.15.2", "jruby.version"=>"jruby 9.2.19.0 (2.5.8) 2021-06-15 55810c552b OpenJDK 64-Bit Server VM 11.0.12+7 on 11.0.12+7 +indy +jit [linux-x86_64]"}
[2023-03-13T21:40:47.179+0000] {subprocess.py:93} INFO - [2023-03-13T21:40:47,178][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2023-03-13T21:40:49.351+0000] {subprocess.py:93} INFO - [2023-03-13T21:40:49,350][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2023-03-13T21:40:49.945+0000] {subprocess.py:93} INFO - [2023-03-13T21:40:49,945][INFO ][org.reflections.Reflections] Reflections took 89 ms to scan 1 urls, producing 120 keys and 417 values
[2023-03-13T21:40:51.337+0000] {subprocess.py:93} INFO - [2023-03-13T21:40:51,336][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2023-03-13T21:40:51.718+0000] {subprocess.py:93} INFO - [2023-03-13T21:40:51,716][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2023-03-13T21:40:51.914+0000] {subprocess.py:93} INFO - [2023-03-13T21:40:51,914][WARN ][logstash.outputs.elasticsearch][main] Restored connection to ES instance {:url=>"http://localhost:9200/"}
[2023-03-13T21:40:51.969+0000] {subprocess.py:93} INFO - [2023-03-13T21:40:51,968][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch version determined (7.15.2) {:es_version=>7}
[2023-03-13T21:40:51.972+0000] {subprocess.py:93} INFO - [2023-03-13T21:40:51,971][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7}
[2023-03-13T21:40:52.148+0000] {subprocess.py:93} INFO - [2023-03-13T21:40:52,147][INFO ][logstash.outputs.elasticsearch][main] Using a default mapping template {:es_version=>7, :ecs_compatibility=>:disabled}
[2023-03-13T21:40:52.170+0000] {subprocess.py:93} INFO - [2023-03-13T21:40:52,170][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1000, "pipeline.sources"=>["/etc/logstash/conf.d/data.conf"], :thread=>"#<Thread:0x365981eb run>"}
[2023-03-13T21:40:53.249+0000] {subprocess.py:93} INFO - [2023-03-13T21:40:53,249][INFO ][logstash.javapipeline    ][main] Pipeline Java execution initialization time {"seconds"=>1.08}
[2023-03-13T21:40:53.361+0000] {subprocess.py:93} INFO - [2023-03-13T21:40:53,360][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2023-03-13T21:40:53.465+0000] {subprocess.py:93} INFO - [2023-03-13T21:40:53,464][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2023-03-13T21:40:54.950+0000] {subprocess.py:93} INFO - [2023-03-13T21:40:54,949][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.023845s) SELECT CAST(current_setting('server_version_num') AS integer) AS v
[2023-03-13T21:40:55.373+0000] {subprocess.py:93} INFO - [2023-03-13T21:40:55,373][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.261425s) SELECT count(*) AS "count" FROM (SELECT * FROM customers) AS "t1" LIMIT 1
[2023-03-13T21:40:55.789+0000] {subprocess.py:93} INFO - [2023-03-13T21:40:55,788][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.388068s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 0
[2023-03-13T21:41:04.390+0000] {subprocess.py:93} INFO - [2023-03-13T21:41:04,390][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.413898s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 50000
[2023-03-13T21:41:09.138+0000] {subprocess.py:93} INFO - [2023-03-13T21:41:09,137][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.261852s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 100000
[2023-03-13T21:41:12.156+0000] {subprocess.py:93} INFO - [2023-03-13T21:41:12,155][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.137636s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 150000
[2023-03-13T21:41:15.212+0000] {subprocess.py:93} INFO - [2023-03-13T21:41:15,212][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.176987s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 200000
[2023-03-13T21:41:18.700+0000] {subprocess.py:93} INFO - [2023-03-13T21:41:18,700][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.118780s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 250000
[2023-03-13T21:41:21.737+0000] {subprocess.py:93} INFO - [2023-03-13T21:41:21,736][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.151780s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 300000
[2023-03-13T21:41:25.049+0000] {subprocess.py:93} INFO - [2023-03-13T21:41:25,049][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.211178s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 350000
[2023-03-13T21:41:28.714+0000] {subprocess.py:93} INFO - [2023-03-13T21:41:28,713][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.222261s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 400000
[2023-03-13T21:41:33.170+0000] {subprocess.py:93} INFO - [2023-03-13T21:41:33,169][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.225731s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 450000
[2023-03-13T21:41:36.770+0000] {subprocess.py:93} INFO - [2023-03-13T21:41:36,770][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.203995s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 500000
[2023-03-13T21:41:39.745+0000] {subprocess.py:93} INFO - [2023-03-13T21:41:39,744][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.114995s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 550000
[2023-03-13T21:41:42.940+0000] {subprocess.py:93} INFO - [2023-03-13T21:41:42,940][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.211278s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 600000
[2023-03-13T21:41:46.355+0000] {subprocess.py:93} INFO - [2023-03-13T21:41:46,355][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.271120s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 650000
[2023-03-13T21:41:51.272+0000] {subprocess.py:93} INFO - [2023-03-13T21:41:51,272][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.219690s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 700000
[2023-03-13T21:41:54.893+0000] {subprocess.py:93} INFO - [2023-03-13T21:41:54,893][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.185330s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 750000
[2023-03-13T21:41:58.928+0000] {subprocess.py:93} INFO - [2023-03-13T21:41:58,928][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.267832s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 800000
[2023-03-13T21:42:02.463+0000] {subprocess.py:93} INFO - [2023-03-13T21:42:02,462][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.169394s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 850000
[2023-03-13T21:42:05.350+0000] {subprocess.py:93} INFO - [2023-03-13T21:42:05,350][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.164278s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 900000
[2023-03-13T21:42:08.586+0000] {subprocess.py:93} INFO - [2023-03-13T21:42:08,586][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.219934s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 950000
[2023-03-13T21:42:12.400+0000] {subprocess.py:93} INFO - [2023-03-13T21:42:12,400][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.172410s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 1000000
[2023-03-13T21:42:15.603+0000] {subprocess.py:93} INFO - [2023-03-13T21:42:15,602][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.186246s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 1050000
[2023-03-13T21:42:18.872+0000] {subprocess.py:93} INFO - [2023-03-13T21:42:18,872][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.169691s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 1100000
[2023-03-13T21:42:21.603+0000] {subprocess.py:93} INFO - [2023-03-13T21:42:21,602][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.139429s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 1150000
[2023-03-13T21:42:25.288+0000] {subprocess.py:93} INFO - [2023-03-13T21:42:25,287][INFO ][logstash.javapipeline    ][main] Pipeline terminated {"pipeline.id"=>"main"}
[2023-03-13T21:42:25.656+0000] {subprocess.py:93} INFO - [2023-03-13T21:42:25,655][INFO ][logstash.pipelinesregistry] Removed pipeline from registry successfully {:pipeline_id=>:main}
[2023-03-13T21:42:25.723+0000] {subprocess.py:93} INFO - [2023-03-13T21:42:25,722][INFO ][logstash.runner          ] Logstash shut down.
[2023-03-13T21:42:26.010+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-03-13T21:42:26.045+0000] {taskinstance.py:1318} INFO - Marking task as SUCCESS. dag_id=AUTOMATISATION, task_id=Chargement_des_données_in_ELK, execution_date=20230313T000209, start_date=20230313T214019, end_date=20230313T214226
[2023-03-13T21:42:26.079+0000] {local_task_job.py:208} INFO - Task exited with return code 0
[2023-03-13T21:42:26.092+0000] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-03-14T10:01:20.599+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:02:09+00:00 [queued]>
[2023-03-14T10:01:20.610+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:02:09+00:00 [queued]>
[2023-03-14T10:01:20.611+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2023-03-14T10:01:20.611+0000] {taskinstance.py:1280} INFO - Starting attempt 1 of 4
[2023-03-14T10:01:20.611+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2023-03-14T10:01:20.630+0000] {taskinstance.py:1300} INFO - Executing <Task(BashOperator): Chargement_des_données_in_ELK> on 2023-03-13 00:02:09+00:00
[2023-03-14T10:01:20.633+0000] {standard_task_runner.py:55} INFO - Started process 34789 to run task
[2023-03-14T10:01:20.637+0000] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'AUTOMATISATION', 'Chargement_des_données_in_ELK', 'scheduled__2023-03-13T00:02:09+00:00', '--job-id', '609', '--raw', '--subdir', 'DAGS_FOLDER/main.py', '--cfg-path', '/tmp/tmpuk4y8_3_']
[2023-03-14T10:01:20.639+0000] {standard_task_runner.py:83} INFO - Job 609: Subtask Chargement_des_données_in_ELK
[2023-03-14T10:01:20.705+0000] {task_command.py:388} INFO - Running <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:02:09+00:00 [running]> on host dev
[2023-03-14T10:01:20.781+0000] {taskinstance.py:1507} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=AUTOMATISATION
AIRFLOW_CTX_TASK_ID=Chargement_des_données_in_ELK
AIRFLOW_CTX_EXECUTION_DATE=2023-03-13T00:02:09+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-13T00:02:09+00:00
[2023-03-14T10:01:20.783+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-03-14T10:01:20.784+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'echo dev | sudo -S /usr/share/logstash/bin/logstash --path.settings=/etc/logstash/ -f /etc/logstash/conf.d/data.conf']
[2023-03-14T10:01:20.792+0000] {subprocess.py:86} INFO - Output:
[2023-03-14T10:01:20.880+0000] {subprocess.py:93} INFO - [sudo] Mot de passe de dev : Using bundled JDK: /usr/share/logstash/jdk
[2023-03-14T10:01:21.131+0000] {subprocess.py:93} INFO - OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
[2023-03-14T10:01:44.929+0000] {subprocess.py:93} INFO - Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties
[2023-03-14T10:01:45.054+0000] {subprocess.py:93} INFO - [2023-03-14T10:01:45,050][INFO ][logstash.runner          ] Log4j configuration path used is: /etc/logstash/log4j2.properties
[2023-03-14T10:01:45.067+0000] {subprocess.py:93} INFO - [2023-03-14T10:01:45,066][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.15.2", "jruby.version"=>"jruby 9.2.19.0 (2.5.8) 2021-06-15 55810c552b OpenJDK 64-Bit Server VM 11.0.12+7 on 11.0.12+7 +indy +jit [linux-x86_64]"}
[2023-03-14T10:01:45.645+0000] {subprocess.py:93} INFO - [2023-03-14T10:01:45,645][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2023-03-14T10:01:47.579+0000] {subprocess.py:93} INFO - [2023-03-14T10:01:47,577][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2023-03-14T10:01:48.399+0000] {subprocess.py:93} INFO - [2023-03-14T10:01:48,399][INFO ][org.reflections.Reflections] Reflections took 131 ms to scan 1 urls, producing 120 keys and 417 values
[2023-03-14T10:01:50.063+0000] {subprocess.py:93} INFO - [2023-03-14T10:01:50,062][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2023-03-14T10:01:50.522+0000] {subprocess.py:93} INFO - [2023-03-14T10:01:50,519][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2023-03-14T10:01:50.765+0000] {subprocess.py:93} INFO - [2023-03-14T10:01:50,764][WARN ][logstash.outputs.elasticsearch][main] Restored connection to ES instance {:url=>"http://localhost:9200/"}
[2023-03-14T10:01:50.832+0000] {subprocess.py:93} INFO - [2023-03-14T10:01:50,832][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch version determined (7.15.2) {:es_version=>7}
[2023-03-14T10:01:50.836+0000] {subprocess.py:93} INFO - [2023-03-14T10:01:50,836][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7}
[2023-03-14T10:01:51.081+0000] {subprocess.py:93} INFO - [2023-03-14T10:01:51,080][INFO ][logstash.outputs.elasticsearch][main] Using a default mapping template {:es_version=>7, :ecs_compatibility=>:disabled}
[2023-03-14T10:01:51.177+0000] {subprocess.py:93} INFO - [2023-03-14T10:01:51,176][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1000, "pipeline.sources"=>["/etc/logstash/conf.d/data.conf"], :thread=>"#<Thread:0x15b04653 run>"}
[2023-03-14T10:01:52.496+0000] {subprocess.py:93} INFO - [2023-03-14T10:01:52,495][INFO ][logstash.javapipeline    ][main] Pipeline Java execution initialization time {"seconds"=>1.31}
[2023-03-14T10:01:52.669+0000] {subprocess.py:93} INFO - [2023-03-14T10:01:52,669][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2023-03-14T10:01:52.793+0000] {subprocess.py:93} INFO - [2023-03-14T10:01:52,792][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2023-03-14T10:01:54.604+0000] {subprocess.py:93} INFO - [2023-03-14T10:01:54,603][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.025839s) SELECT CAST(current_setting('server_version_num') AS integer) AS v
[2023-03-14T10:01:54.861+0000] {subprocess.py:93} INFO - [2023-03-14T10:01:54,861][ERROR][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] Java::OrgPostgresqlUtil::PSQLException: ERROR: relation "customers" does not exist
[2023-03-14T10:01:54.862+0000] {subprocess.py:93} INFO -   Position : 48: SELECT count(*) AS "count" FROM (SELECT * FROM customers) AS "t1" LIMIT 1
[2023-03-14T10:01:54.900+0000] {subprocess.py:93} INFO - [2023-03-14T10:01:54,899][WARN ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] Exception when executing JDBC query {:exception=>"Java::OrgPostgresqlUtil::PSQLException: ERROR: relation \"customers\" does not exist\n  Position : 48"}
[2023-03-14T10:01:55.948+0000] {subprocess.py:93} INFO - [2023-03-14T10:01:55,947][INFO ][logstash.javapipeline    ][main] Pipeline terminated {"pipeline.id"=>"main"}
[2023-03-14T10:01:56.380+0000] {subprocess.py:93} INFO - [2023-03-14T10:01:56,380][INFO ][logstash.pipelinesregistry] Removed pipeline from registry successfully {:pipeline_id=>:main}
[2023-03-14T10:01:56.432+0000] {subprocess.py:93} INFO - [2023-03-14T10:01:56,432][INFO ][logstash.runner          ] Logstash shut down.
[2023-03-14T10:01:56.564+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-03-14T10:01:56.596+0000] {taskinstance.py:1318} INFO - Marking task as SUCCESS. dag_id=AUTOMATISATION, task_id=Chargement_des_données_in_ELK, execution_date=20230313T000209, start_date=20230314T100120, end_date=20230314T100156
[2023-03-14T10:01:56.642+0000] {local_task_job.py:208} INFO - Task exited with return code 0
[2023-03-14T10:01:56.658+0000] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-03-15T07:22:18.142+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:02:09+00:00 [queued]>
[2023-03-15T07:22:18.149+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:02:09+00:00 [queued]>
[2023-03-15T07:22:18.149+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2023-03-15T07:22:18.149+0000] {taskinstance.py:1280} INFO - Starting attempt 1 of 4
[2023-03-15T07:22:18.149+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2023-03-15T07:22:18.163+0000] {taskinstance.py:1300} INFO - Executing <Task(BashOperator): Chargement_des_données_in_ELK> on 2023-03-13 00:02:09+00:00
[2023-03-15T07:22:18.165+0000] {standard_task_runner.py:55} INFO - Started process 124663 to run task
[2023-03-15T07:22:18.167+0000] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'AUTOMATISATION', 'Chargement_des_données_in_ELK', 'scheduled__2023-03-13T00:02:09+00:00', '--job-id', '734', '--raw', '--subdir', 'DAGS_FOLDER/main.py', '--cfg-path', '/tmp/tmpfir9eo9h']
[2023-03-15T07:22:18.168+0000] {standard_task_runner.py:83} INFO - Job 734: Subtask Chargement_des_données_in_ELK
[2023-03-15T07:22:18.204+0000] {task_command.py:388} INFO - Running <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:02:09+00:00 [running]> on host dev
[2023-03-15T07:22:18.251+0000] {taskinstance.py:1507} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=AUTOMATISATION
AIRFLOW_CTX_TASK_ID=Chargement_des_données_in_ELK
AIRFLOW_CTX_EXECUTION_DATE=2023-03-13T00:02:09+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-13T00:02:09+00:00
[2023-03-15T07:22:18.252+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-03-15T07:22:18.252+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'echo dev | sudo -S /usr/share/logstash/bin/logstash --path.settings=/etc/logstash/ -f /etc/logstash/conf.d/data.conf']
[2023-03-15T07:22:18.257+0000] {subprocess.py:86} INFO - Output:
[2023-03-15T07:22:18.303+0000] {subprocess.py:93} INFO - [sudo] Mot de passe de dev : Using bundled JDK: /usr/share/logstash/jdk
[2023-03-15T07:22:18.448+0000] {subprocess.py:93} INFO - OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
[2023-03-15T07:22:35.944+0000] {subprocess.py:93} INFO - Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties
[2023-03-15T07:22:36.041+0000] {subprocess.py:93} INFO - [2023-03-15T07:22:36,038][INFO ][logstash.runner          ] Log4j configuration path used is: /etc/logstash/log4j2.properties
[2023-03-15T07:22:36.049+0000] {subprocess.py:93} INFO - [2023-03-15T07:22:36,049][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.15.2", "jruby.version"=>"jruby 9.2.19.0 (2.5.8) 2021-06-15 55810c552b OpenJDK 64-Bit Server VM 11.0.12+7 on 11.0.12+7 +indy +jit [linux-x86_64]"}
[2023-03-15T07:22:36.343+0000] {subprocess.py:93} INFO - [2023-03-15T07:22:36,342][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2023-03-15T07:22:37.390+0000] {subprocess.py:93} INFO - [2023-03-15T07:22:37,389][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2023-03-15T07:22:37.950+0000] {subprocess.py:93} INFO - [2023-03-15T07:22:37,949][INFO ][org.reflections.Reflections] Reflections took 86 ms to scan 1 urls, producing 120 keys and 417 values
[2023-03-15T07:22:38.948+0000] {subprocess.py:93} INFO - [2023-03-15T07:22:38,947][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2023-03-15T07:22:39.283+0000] {subprocess.py:93} INFO - [2023-03-15T07:22:39,282][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2023-03-15T07:22:39.454+0000] {subprocess.py:93} INFO - [2023-03-15T07:22:39,454][WARN ][logstash.outputs.elasticsearch][main] Restored connection to ES instance {:url=>"http://localhost:9200/"}
[2023-03-15T07:22:39.506+0000] {subprocess.py:93} INFO - [2023-03-15T07:22:39,506][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch version determined (7.15.2) {:es_version=>7}
[2023-03-15T07:22:39.510+0000] {subprocess.py:93} INFO - [2023-03-15T07:22:39,510][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7}
[2023-03-15T07:22:39.641+0000] {subprocess.py:93} INFO - [2023-03-15T07:22:39,641][INFO ][logstash.outputs.elasticsearch][main] Using a default mapping template {:es_version=>7, :ecs_compatibility=>:disabled}
[2023-03-15T07:22:39.693+0000] {subprocess.py:93} INFO - [2023-03-15T07:22:39,693][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1000, "pipeline.sources"=>["/etc/logstash/conf.d/data.conf"], :thread=>"#<Thread:0x5938118e run>"}
[2023-03-15T07:22:40.381+0000] {subprocess.py:93} INFO - [2023-03-15T07:22:40,381][INFO ][logstash.javapipeline    ][main] Pipeline Java execution initialization time {"seconds"=>0.69}
[2023-03-15T07:22:40.468+0000] {subprocess.py:93} INFO - [2023-03-15T07:22:40,468][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2023-03-15T07:22:40.552+0000] {subprocess.py:93} INFO - [2023-03-15T07:22:40,551][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2023-03-15T07:22:41.522+0000] {subprocess.py:93} INFO - [2023-03-15T07:22:41,521][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.016981s) SELECT CAST(current_setting('server_version_num') AS integer) AS v
[2023-03-15T07:22:41.663+0000] {subprocess.py:93} INFO - [2023-03-15T07:22:41,663][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.005680s) SELECT count(*) AS "count" FROM (SELECT * FROM customers) AS "t1" LIMIT 1
[2023-03-15T07:22:41.876+0000] {subprocess.py:93} INFO - [2023-03-15T07:22:41,876][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.194416s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 0
[2023-03-15T07:22:46.585+0000] {subprocess.py:93} INFO - [2023-03-15T07:22:46,584][INFO ][logstash.javapipeline    ][main] Pipeline terminated {"pipeline.id"=>"main"}
[2023-03-15T07:22:46.636+0000] {subprocess.py:93} INFO - [2023-03-15T07:22:46,636][INFO ][logstash.pipelinesregistry] Removed pipeline from registry successfully {:pipeline_id=>:main}
[2023-03-15T07:22:46.691+0000] {subprocess.py:93} INFO - [2023-03-15T07:22:46,690][INFO ][logstash.runner          ] Logstash shut down.
[2023-03-15T07:22:46.923+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-03-15T07:22:46.942+0000] {taskinstance.py:1318} INFO - Marking task as SUCCESS. dag_id=AUTOMATISATION, task_id=Chargement_des_données_in_ELK, execution_date=20230313T000209, start_date=20230315T072218, end_date=20230315T072246
[2023-03-15T07:22:46.977+0000] {local_task_job.py:208} INFO - Task exited with return code 0
[2023-03-15T07:22:46.985+0000] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
