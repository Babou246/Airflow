[2023-03-13T22:40:32.425+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:03:36+00:00 [queued]>
[2023-03-13T22:40:32.437+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:03:36+00:00 [queued]>
[2023-03-13T22:40:32.438+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2023-03-13T22:40:32.438+0000] {taskinstance.py:1280} INFO - Starting attempt 1 of 4
[2023-03-13T22:40:32.438+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2023-03-13T22:40:32.471+0000] {taskinstance.py:1300} INFO - Executing <Task(BashOperator): Chargement_des_données_in_ELK> on 2023-03-13 00:03:36+00:00
[2023-03-13T22:40:32.474+0000] {standard_task_runner.py:55} INFO - Started process 75749 to run task
[2023-03-13T22:40:32.479+0000] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'AUTOMATISATION', 'Chargement_des_données_in_ELK', 'scheduled__2023-03-13T00:03:36+00:00', '--job-id', '423', '--raw', '--subdir', 'DAGS_FOLDER/main.py', '--cfg-path', '/tmp/tmpxtcwmxny']
[2023-03-13T22:40:32.481+0000] {standard_task_runner.py:83} INFO - Job 423: Subtask Chargement_des_données_in_ELK
[2023-03-13T22:40:32.558+0000] {task_command.py:388} INFO - Running <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:03:36+00:00 [running]> on host dev
[2023-03-13T22:40:32.657+0000] {taskinstance.py:1507} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=AUTOMATISATION
AIRFLOW_CTX_TASK_ID=Chargement_des_données_in_ELK
AIRFLOW_CTX_EXECUTION_DATE=2023-03-13T00:03:36+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-13T00:03:36+00:00
[2023-03-13T22:40:32.659+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-03-13T22:40:32.660+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'echo dev | sudo -S /usr/share/logstash/bin/logstash --path.settings=/etc/logstash/ -f /etc/logstash/conf.d/data.conf']
[2023-03-13T22:40:32.668+0000] {subprocess.py:86} INFO - Output:
[2023-03-13T22:40:32.765+0000] {subprocess.py:93} INFO - [sudo] Mot de passe de dev : Using bundled JDK: /usr/share/logstash/jdk
[2023-03-13T22:40:33.039+0000] {subprocess.py:93} INFO - OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
[2023-03-13T22:40:59.367+0000] {subprocess.py:93} INFO - Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties
[2023-03-13T22:40:59.757+0000] {subprocess.py:93} INFO - [2023-03-13T22:40:59,749][INFO ][logstash.runner          ] Log4j configuration path used is: /etc/logstash/log4j2.properties
[2023-03-13T22:40:59.800+0000] {subprocess.py:93} INFO - [2023-03-13T22:40:59,799][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.15.2", "jruby.version"=>"jruby 9.2.19.0 (2.5.8) 2021-06-15 55810c552b OpenJDK 64-Bit Server VM 11.0.12+7 on 11.0.12+7 +indy +jit [linux-x86_64]"}
[2023-03-13T22:41:00.478+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:00,478][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2023-03-13T22:41:02.543+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:02,541][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2023-03-13T22:41:03.308+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:03,308][INFO ][org.reflections.Reflections] Reflections took 140 ms to scan 1 urls, producing 120 keys and 417 values
[2023-03-13T22:41:04.977+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:04,976][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2023-03-13T22:41:05.540+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:05,538][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2023-03-13T22:41:05.727+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:05,727][WARN ][logstash.outputs.elasticsearch][main] Restored connection to ES instance {:url=>"http://localhost:9200/"}
[2023-03-13T22:41:05.788+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:05,788][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch version determined (7.15.2) {:es_version=>7}
[2023-03-13T22:41:05.792+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:05,792][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7}
[2023-03-13T22:41:05.984+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:05,983][INFO ][logstash.outputs.elasticsearch][main] Using a default mapping template {:es_version=>7, :ecs_compatibility=>:disabled}
[2023-03-13T22:41:06.056+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:06,055][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1000, "pipeline.sources"=>["/etc/logstash/conf.d/data.conf"], :thread=>"#<Thread:0x50d75d8c run>"}
[2023-03-13T22:41:07.443+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:07,442][INFO ][logstash.javapipeline    ][main] Pipeline Java execution initialization time {"seconds"=>1.38}
[2023-03-13T22:41:07.583+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:07,582][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2023-03-13T22:41:07.688+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:07,687][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2023-03-13T22:41:09.481+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:09,480][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.026074s) SELECT CAST(current_setting('server_version_num') AS integer) AS v
[2023-03-13T22:41:09.792+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:09,792][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.127981s) SELECT count(*) AS "count" FROM (SELECT * FROM customers) AS "t1" LIMIT 1
[2023-03-13T22:41:10.288+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:10,288][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.453675s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 0
[2023-03-13T22:41:22.641+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:22,641][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.496125s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 50000
[2023-03-13T22:41:29.115+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:29,115][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.377330s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 100000
[2023-03-13T22:41:35.277+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:35,276][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.148146s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 150000
[2023-03-13T22:41:38.672+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:38,671][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.195382s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 200000
[2023-03-13T22:41:42.024+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:42,024][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.139667s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 250000
[2023-03-13T22:41:45.905+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:45,904][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.203914s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 300000
[2023-03-13T22:41:51.145+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:51,145][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.227177s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 350000
[2023-03-13T22:41:54.397+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:54,397][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.136389s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 400000
[2023-03-13T22:41:58.108+0000] {subprocess.py:93} INFO - [2023-03-13T22:41:58,108][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.218233s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 450000
[2023-03-13T22:42:00.999+0000] {subprocess.py:93} INFO - [2023-03-13T22:42:00,999][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.186381s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 500000
[2023-03-13T22:42:04.090+0000] {subprocess.py:93} INFO - [2023-03-13T22:42:04,090][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.204166s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 550000
[2023-03-13T22:42:07.135+0000] {subprocess.py:93} INFO - [2023-03-13T22:42:07,135][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.238785s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 600000
[2023-03-13T22:42:10.567+0000] {subprocess.py:93} INFO - [2023-03-13T22:42:10,566][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.191199s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 650000
[2023-03-13T22:42:13.461+0000] {subprocess.py:93} INFO - [2023-03-13T22:42:13,460][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.243536s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 700000
[2023-03-13T22:42:18.073+0000] {subprocess.py:93} INFO - [2023-03-13T22:42:18,073][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.186917s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 750000
[2023-03-13T22:42:20.729+0000] {subprocess.py:93} INFO - [2023-03-13T22:42:20,729][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.149014s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 800000
[2023-03-13T22:42:24.232+0000] {subprocess.py:93} INFO - [2023-03-13T22:42:24,231][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.234432s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 850000
[2023-03-13T22:42:27.602+0000] {subprocess.py:93} INFO - [2023-03-13T22:42:27,602][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.239388s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 900000
[2023-03-13T22:42:30.522+0000] {subprocess.py:93} INFO - [2023-03-13T22:42:30,522][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.204474s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 950000
[2023-03-13T22:42:33.419+0000] {subprocess.py:93} INFO - [2023-03-13T22:42:33,419][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.157932s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 1000000
[2023-03-13T22:42:35.901+0000] {subprocess.py:93} INFO - [2023-03-13T22:42:35,901][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.155163s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 1050000
[2023-03-13T22:42:38.829+0000] {subprocess.py:93} INFO - [2023-03-13T22:42:38,829][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.197365s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 1100000
[2023-03-13T22:42:41.612+0000] {subprocess.py:93} INFO - [2023-03-13T22:42:41,611][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.232343s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 1150000
[2023-03-13T22:42:47.173+0000] {subprocess.py:93} INFO - [2023-03-13T22:42:47,172][INFO ][logstash.javapipeline    ][main] Pipeline terminated {"pipeline.id"=>"main"}
[2023-03-13T22:42:47.274+0000] {subprocess.py:93} INFO - [2023-03-13T22:42:47,273][INFO ][logstash.pipelinesregistry] Removed pipeline from registry successfully {:pipeline_id=>:main}
[2023-03-13T22:42:47.490+0000] {subprocess.py:93} INFO - [2023-03-13T22:42:47,489][INFO ][logstash.runner          ] Logstash shut down.
[2023-03-13T22:42:47.705+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-03-13T22:42:47.859+0000] {taskinstance.py:1318} INFO - Marking task as SUCCESS. dag_id=AUTOMATISATION, task_id=Chargement_des_données_in_ELK, execution_date=20230313T000336, start_date=20230313T224032, end_date=20230313T224247
[2023-03-13T22:42:47.983+0000] {local_task_job.py:208} INFO - Task exited with return code 0
[2023-03-13T22:42:47.998+0000] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-03-15T07:33:59.007+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:03:36+00:00 [queued]>
[2023-03-15T07:33:59.011+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:03:36+00:00 [queued]>
[2023-03-15T07:33:59.012+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2023-03-15T07:33:59.012+0000] {taskinstance.py:1280} INFO - Starting attempt 1 of 4
[2023-03-15T07:33:59.012+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2023-03-15T07:33:59.022+0000] {taskinstance.py:1300} INFO - Executing <Task(BashOperator): Chargement_des_données_in_ELK> on 2023-03-13 00:03:36+00:00
[2023-03-15T07:33:59.023+0000] {standard_task_runner.py:55} INFO - Started process 132521 to run task
[2023-03-15T07:33:59.025+0000] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'AUTOMATISATION', 'Chargement_des_données_in_ELK', 'scheduled__2023-03-13T00:03:36+00:00', '--job-id', '792', '--raw', '--subdir', 'DAGS_FOLDER/main.py', '--cfg-path', '/tmp/tmp6z7lgyzb']
[2023-03-15T07:33:59.025+0000] {standard_task_runner.py:83} INFO - Job 792: Subtask Chargement_des_données_in_ELK
[2023-03-15T07:33:59.055+0000] {task_command.py:388} INFO - Running <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:03:36+00:00 [running]> on host dev
[2023-03-15T07:33:59.094+0000] {taskinstance.py:1507} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=AUTOMATISATION
AIRFLOW_CTX_TASK_ID=Chargement_des_données_in_ELK
AIRFLOW_CTX_EXECUTION_DATE=2023-03-13T00:03:36+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-13T00:03:36+00:00
[2023-03-15T07:33:59.095+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-03-15T07:33:59.095+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'echo dev | sudo -S /usr/share/logstash/bin/logstash --path.settings=/etc/logstash/ -f /etc/logstash/conf.d/data.conf']
[2023-03-15T07:33:59.099+0000] {subprocess.py:86} INFO - Output:
[2023-03-15T07:33:59.135+0000] {subprocess.py:93} INFO - [sudo] Mot de passe de dev : Using bundled JDK: /usr/share/logstash/jdk
[2023-03-15T07:33:59.274+0000] {subprocess.py:93} INFO - OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
[2023-03-15T07:34:11.470+0000] {subprocess.py:93} INFO - Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties
[2023-03-15T07:34:11.527+0000] {subprocess.py:93} INFO - [2023-03-15T07:34:11,526][INFO ][logstash.runner          ] Log4j configuration path used is: /etc/logstash/log4j2.properties
[2023-03-15T07:34:11.533+0000] {subprocess.py:93} INFO - [2023-03-15T07:34:11,533][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.15.2", "jruby.version"=>"jruby 9.2.19.0 (2.5.8) 2021-06-15 55810c552b OpenJDK 64-Bit Server VM 11.0.12+7 on 11.0.12+7 +indy +jit [linux-x86_64]"}
[2023-03-15T07:34:11.773+0000] {subprocess.py:93} INFO - [2023-03-15T07:34:11,772][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2023-03-15T07:34:12.628+0000] {subprocess.py:93} INFO - [2023-03-15T07:34:12,628][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2023-03-15T07:34:13.113+0000] {subprocess.py:93} INFO - [2023-03-15T07:34:13,112][INFO ][org.reflections.Reflections] Reflections took 65 ms to scan 1 urls, producing 120 keys and 417 values
[2023-03-15T07:34:14.039+0000] {subprocess.py:93} INFO - [2023-03-15T07:34:14,038][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2023-03-15T07:34:14.298+0000] {subprocess.py:93} INFO - [2023-03-15T07:34:14,297][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2023-03-15T07:34:14.409+0000] {subprocess.py:93} INFO - [2023-03-15T07:34:14,409][WARN ][logstash.outputs.elasticsearch][main] Restored connection to ES instance {:url=>"http://localhost:9200/"}
[2023-03-15T07:34:14.443+0000] {subprocess.py:93} INFO - [2023-03-15T07:34:14,442][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch version determined (7.15.2) {:es_version=>7}
[2023-03-15T07:34:14.445+0000] {subprocess.py:93} INFO - [2023-03-15T07:34:14,445][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7}
[2023-03-15T07:34:14.560+0000] {subprocess.py:93} INFO - [2023-03-15T07:34:14,559][INFO ][logstash.outputs.elasticsearch][main] Using a default mapping template {:es_version=>7, :ecs_compatibility=>:disabled}
[2023-03-15T07:34:14.585+0000] {subprocess.py:93} INFO - [2023-03-15T07:34:14,585][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1000, "pipeline.sources"=>["/etc/logstash/conf.d/data.conf"], :thread=>"#<Thread:0x11f2bd03 run>"}
[2023-03-15T07:34:15.219+0000] {subprocess.py:93} INFO - [2023-03-15T07:34:15,219][INFO ][logstash.javapipeline    ][main] Pipeline Java execution initialization time {"seconds"=>0.63}
[2023-03-15T07:34:15.297+0000] {subprocess.py:93} INFO - [2023-03-15T07:34:15,297][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2023-03-15T07:34:15.355+0000] {subprocess.py:93} INFO - [2023-03-15T07:34:15,355][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2023-03-15T07:34:16.071+0000] {subprocess.py:93} INFO - [2023-03-15T07:34:16,071][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.011460s) SELECT CAST(current_setting('server_version_num') AS integer) AS v
[2023-03-15T07:34:16.147+0000] {subprocess.py:93} INFO - [2023-03-15T07:34:16,146][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.001344s) SELECT count(*) AS "count" FROM (SELECT * FROM customers) AS "t1" LIMIT 1
[2023-03-15T07:34:16.494+0000] {subprocess.py:93} INFO - [2023-03-15T07:34:16,493][INFO ][logstash.javapipeline    ][main] Pipeline terminated {"pipeline.id"=>"main"}
[2023-03-15T07:34:16.896+0000] {subprocess.py:93} INFO - [2023-03-15T07:34:16,896][INFO ][logstash.pipelinesregistry] Removed pipeline from registry successfully {:pipeline_id=>:main}
[2023-03-15T07:34:16.915+0000] {subprocess.py:93} INFO - [2023-03-15T07:34:16,914][INFO ][logstash.runner          ] Logstash shut down.
[2023-03-15T07:34:16.958+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-03-15T07:34:16.972+0000] {taskinstance.py:1318} INFO - Marking task as SUCCESS. dag_id=AUTOMATISATION, task_id=Chargement_des_données_in_ELK, execution_date=20230313T000336, start_date=20230315T073359, end_date=20230315T073416
[2023-03-15T07:34:17.007+0000] {local_task_job.py:208} INFO - Task exited with return code 0
[2023-03-15T07:34:17.020+0000] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
