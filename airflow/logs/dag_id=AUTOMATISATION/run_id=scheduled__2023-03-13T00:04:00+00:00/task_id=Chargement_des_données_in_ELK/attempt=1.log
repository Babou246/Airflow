[2023-03-13T22:57:35.827+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:04:00+00:00 [queued]>
[2023-03-13T22:57:35.840+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:04:00+00:00 [queued]>
[2023-03-13T22:57:35.840+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2023-03-13T22:57:35.841+0000] {taskinstance.py:1280} INFO - Starting attempt 1 of 4
[2023-03-13T22:57:35.841+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2023-03-13T22:57:35.870+0000] {taskinstance.py:1300} INFO - Executing <Task(BashOperator): Chargement_des_données_in_ELK> on 2023-03-13 00:04:00+00:00
[2023-03-13T22:57:35.874+0000] {standard_task_runner.py:55} INFO - Started process 80100 to run task
[2023-03-13T22:57:35.878+0000] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'AUTOMATISATION', 'Chargement_des_données_in_ELK', 'scheduled__2023-03-13T00:04:00+00:00', '--job-id', '439', '--raw', '--subdir', 'DAGS_FOLDER/main.py', '--cfg-path', '/tmp/tmpgolisb73']
[2023-03-13T22:57:35.880+0000] {standard_task_runner.py:83} INFO - Job 439: Subtask Chargement_des_données_in_ELK
[2023-03-13T22:57:35.953+0000] {task_command.py:388} INFO - Running <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:04:00+00:00 [running]> on host dev
[2023-03-13T22:57:36.061+0000] {taskinstance.py:1507} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=AUTOMATISATION
AIRFLOW_CTX_TASK_ID=Chargement_des_données_in_ELK
AIRFLOW_CTX_EXECUTION_DATE=2023-03-13T00:04:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-13T00:04:00+00:00
[2023-03-13T22:57:36.063+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-03-13T22:57:36.063+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'echo dev | sudo -S /usr/share/logstash/bin/logstash --path.settings=/etc/logstash/ -f /etc/logstash/conf.d/data.conf']
[2023-03-13T22:57:36.072+0000] {subprocess.py:86} INFO - Output:
[2023-03-13T22:57:36.165+0000] {subprocess.py:93} INFO - [sudo] Mot de passe de dev : Using bundled JDK: /usr/share/logstash/jdk
[2023-03-13T22:57:36.425+0000] {subprocess.py:93} INFO - OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
[2023-03-13T22:58:02.335+0000] {subprocess.py:93} INFO - Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties
[2023-03-13T22:58:02.574+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:02,570][INFO ][logstash.runner          ] Log4j configuration path used is: /etc/logstash/log4j2.properties
[2023-03-13T22:58:02.588+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:02,587][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.15.2", "jruby.version"=>"jruby 9.2.19.0 (2.5.8) 2021-06-15 55810c552b OpenJDK 64-Bit Server VM 11.0.12+7 on 11.0.12+7 +indy +jit [linux-x86_64]"}
[2023-03-13T22:58:03.140+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:03,139][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2023-03-13T22:58:05.358+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:05,357][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2023-03-13T22:58:06.506+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:06,506][INFO ][org.reflections.Reflections] Reflections took 230 ms to scan 1 urls, producing 120 keys and 417 values
[2023-03-13T22:58:08.198+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:08,198][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2023-03-13T22:58:08.724+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:08,716][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2023-03-13T22:58:08.968+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:08,968][WARN ][logstash.outputs.elasticsearch][main] Restored connection to ES instance {:url=>"http://localhost:9200/"}
[2023-03-13T22:58:09.027+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:09,027][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch version determined (7.15.2) {:es_version=>7}
[2023-03-13T22:58:09.031+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:09,031][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7}
[2023-03-13T22:58:09.321+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:09,320][INFO ][logstash.outputs.elasticsearch][main] Using a default mapping template {:es_version=>7, :ecs_compatibility=>:disabled}
[2023-03-13T22:58:09.443+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:09,443][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1000, "pipeline.sources"=>["/etc/logstash/conf.d/data.conf"], :thread=>"#<Thread:0x455abaee run>"}
[2023-03-13T22:58:10.647+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:10,647][INFO ][logstash.javapipeline    ][main] Pipeline Java execution initialization time {"seconds"=>1.2}
[2023-03-13T22:58:10.776+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:10,776][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2023-03-13T22:58:10.910+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:10,909][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2023-03-13T22:58:12.468+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:12,468][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.024457s) SELECT CAST(current_setting('server_version_num') AS integer) AS v
[2023-03-13T22:58:12.723+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:12,723][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.099214s) SELECT count(*) AS "count" FROM (SELECT * FROM customers) AS "t1" LIMIT 1
[2023-03-13T22:58:13.265+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:13,264][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.508600s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 0
[2023-03-13T22:58:21.783+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:21,782][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.424200s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 50000
[2023-03-13T22:58:26.603+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:26,603][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.198587s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 100000
[2023-03-13T22:58:29.796+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:29,795][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.103599s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 150000
[2023-03-13T22:58:32.600+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:32,600][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.164280s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 200000
[2023-03-13T22:58:36.388+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:36,387][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.158891s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 250000
[2023-03-13T22:58:39.680+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:39,680][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.123758s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 300000
[2023-03-13T22:58:42.649+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:42,649][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.141861s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 350000
[2023-03-13T22:58:45.209+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:45,209][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.141883s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 400000
[2023-03-13T22:58:47.981+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:47,980][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.129044s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 450000
[2023-03-13T22:58:51.050+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:51,050][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.122698s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 500000
[2023-03-13T22:58:53.634+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:53,634][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.131081s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 550000
[2023-03-13T22:58:57.138+0000] {subprocess.py:93} INFO - [2023-03-13T22:58:57,138][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.169495s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 600000
[2023-03-13T22:59:00.281+0000] {subprocess.py:93} INFO - [2023-03-13T22:59:00,281][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.213169s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 650000
[2023-03-13T22:59:04.877+0000] {subprocess.py:93} INFO - [2023-03-13T22:59:04,876][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.182345s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 700000
[2023-03-13T22:59:07.645+0000] {subprocess.py:93} INFO - [2023-03-13T22:59:07,644][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.181053s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 750000
[2023-03-13T22:59:09.985+0000] {subprocess.py:93} INFO - [2023-03-13T22:59:09,985][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.157711s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 800000
[2023-03-13T22:59:12.830+0000] {subprocess.py:93} INFO - [2023-03-13T22:59:12,830][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.192618s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 850000
[2023-03-13T22:59:16.411+0000] {subprocess.py:93} INFO - [2023-03-13T22:59:16,410][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.209885s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 900000
[2023-03-13T22:59:21.351+0000] {subprocess.py:93} INFO - [2023-03-13T22:59:21,350][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.233691s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 950000
[2023-03-13T22:59:24.980+0000] {subprocess.py:93} INFO - [2023-03-13T22:59:24,980][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.256793s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 1000000
[2023-03-13T22:59:28.329+0000] {subprocess.py:93} INFO - [2023-03-13T22:59:28,329][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.200355s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 1050000
[2023-03-13T22:59:38.806+0000] {subprocess.py:93} INFO - [2023-03-13T22:59:38,806][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.238422s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 1100000
[2023-03-13T22:59:42.348+0000] {subprocess.py:93} INFO - [2023-03-13T22:59:42,347][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.160058s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 1150000
[2023-03-13T22:59:45.334+0000] {subprocess.py:93} INFO - [2023-03-13T22:59:45,333][INFO ][logstash.javapipeline    ][main] Pipeline terminated {"pipeline.id"=>"main"}
[2023-03-13T22:59:45.521+0000] {subprocess.py:93} INFO - [2023-03-13T22:59:45,520][INFO ][logstash.pipelinesregistry] Removed pipeline from registry successfully {:pipeline_id=>:main}
[2023-03-13T22:59:45.578+0000] {subprocess.py:93} INFO - [2023-03-13T22:59:45,577][INFO ][logstash.runner          ] Logstash shut down.
[2023-03-13T22:59:45.763+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-03-13T22:59:45.806+0000] {taskinstance.py:1318} INFO - Marking task as SUCCESS. dag_id=AUTOMATISATION, task_id=Chargement_des_données_in_ELK, execution_date=20230313T000400, start_date=20230313T225735, end_date=20230313T225945
[2023-03-13T22:59:45.852+0000] {local_task_job.py:208} INFO - Task exited with return code 0
[2023-03-13T22:59:45.870+0000] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-03-15T07:36:37.481+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:04:00+00:00 [queued]>
[2023-03-15T07:36:37.486+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:04:00+00:00 [queued]>
[2023-03-15T07:36:37.486+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2023-03-15T07:36:37.486+0000] {taskinstance.py:1280} INFO - Starting attempt 1 of 4
[2023-03-15T07:36:37.486+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2023-03-15T07:36:37.496+0000] {taskinstance.py:1300} INFO - Executing <Task(BashOperator): Chargement_des_données_in_ELK> on 2023-03-13 00:04:00+00:00
[2023-03-15T07:36:37.498+0000] {standard_task_runner.py:55} INFO - Started process 133993 to run task
[2023-03-15T07:36:37.499+0000] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'AUTOMATISATION', 'Chargement_des_données_in_ELK', 'scheduled__2023-03-13T00:04:00+00:00', '--job-id', '808', '--raw', '--subdir', 'DAGS_FOLDER/main.py', '--cfg-path', '/tmp/tmpviqnc6_y']
[2023-03-15T07:36:37.500+0000] {standard_task_runner.py:83} INFO - Job 808: Subtask Chargement_des_données_in_ELK
[2023-03-15T07:36:37.530+0000] {task_command.py:388} INFO - Running <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:04:00+00:00 [running]> on host dev
[2023-03-15T07:36:37.565+0000] {taskinstance.py:1507} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=AUTOMATISATION
AIRFLOW_CTX_TASK_ID=Chargement_des_données_in_ELK
AIRFLOW_CTX_EXECUTION_DATE=2023-03-13T00:04:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-13T00:04:00+00:00
[2023-03-15T07:36:37.566+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-03-15T07:36:37.567+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'echo dev | sudo -S /usr/share/logstash/bin/logstash --path.settings=/etc/logstash/ -f /etc/logstash/conf.d/data.conf']
[2023-03-15T07:36:37.571+0000] {subprocess.py:86} INFO - Output:
[2023-03-15T07:36:37.609+0000] {subprocess.py:93} INFO - [sudo] Mot de passe de dev : Using bundled JDK: /usr/share/logstash/jdk
[2023-03-15T07:36:37.740+0000] {subprocess.py:93} INFO - OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
[2023-03-15T07:36:49.370+0000] {subprocess.py:93} INFO - Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties
[2023-03-15T07:36:49.428+0000] {subprocess.py:93} INFO - [2023-03-15T07:36:49,426][INFO ][logstash.runner          ] Log4j configuration path used is: /etc/logstash/log4j2.properties
[2023-03-15T07:36:49.434+0000] {subprocess.py:93} INFO - [2023-03-15T07:36:49,433][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.15.2", "jruby.version"=>"jruby 9.2.19.0 (2.5.8) 2021-06-15 55810c552b OpenJDK 64-Bit Server VM 11.0.12+7 on 11.0.12+7 +indy +jit [linux-x86_64]"}
[2023-03-15T07:36:49.668+0000] {subprocess.py:93} INFO - [2023-03-15T07:36:49,668][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2023-03-15T07:36:50.465+0000] {subprocess.py:93} INFO - [2023-03-15T07:36:50,464][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2023-03-15T07:36:50.899+0000] {subprocess.py:93} INFO - [2023-03-15T07:36:50,899][INFO ][org.reflections.Reflections] Reflections took 84 ms to scan 1 urls, producing 120 keys and 417 values
[2023-03-15T07:36:51.651+0000] {subprocess.py:93} INFO - [2023-03-15T07:36:51,651][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2023-03-15T07:36:51.859+0000] {subprocess.py:93} INFO - [2023-03-15T07:36:51,858][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2023-03-15T07:36:51.957+0000] {subprocess.py:93} INFO - [2023-03-15T07:36:51,957][WARN ][logstash.outputs.elasticsearch][main] Restored connection to ES instance {:url=>"http://localhost:9200/"}
[2023-03-15T07:36:51.988+0000] {subprocess.py:93} INFO - [2023-03-15T07:36:51,987][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch version determined (7.15.2) {:es_version=>7}
[2023-03-15T07:36:51.990+0000] {subprocess.py:93} INFO - [2023-03-15T07:36:51,989][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7}
[2023-03-15T07:36:52.090+0000] {subprocess.py:93} INFO - [2023-03-15T07:36:52,090][INFO ][logstash.outputs.elasticsearch][main] Using a default mapping template {:es_version=>7, :ecs_compatibility=>:disabled}
[2023-03-15T07:36:52.107+0000] {subprocess.py:93} INFO - [2023-03-15T07:36:52,106][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1000, "pipeline.sources"=>["/etc/logstash/conf.d/data.conf"], :thread=>"#<Thread:0x71655807 run>"}
[2023-03-15T07:36:52.589+0000] {subprocess.py:93} INFO - [2023-03-15T07:36:52,589][INFO ][logstash.javapipeline    ][main] Pipeline Java execution initialization time {"seconds"=>0.48}
[2023-03-15T07:36:52.647+0000] {subprocess.py:93} INFO - [2023-03-15T07:36:52,647][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2023-03-15T07:36:52.706+0000] {subprocess.py:93} INFO - [2023-03-15T07:36:52,705][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2023-03-15T07:36:53.445+0000] {subprocess.py:93} INFO - [2023-03-15T07:36:53,445][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.013551s) SELECT CAST(current_setting('server_version_num') AS integer) AS v
[2023-03-15T07:36:53.526+0000] {subprocess.py:93} INFO - [2023-03-15T07:36:53,526][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.002014s) SELECT count(*) AS "count" FROM (SELECT * FROM customers) AS "t1" LIMIT 1
[2023-03-15T07:36:54.039+0000] {subprocess.py:93} INFO - [2023-03-15T07:36:54,039][INFO ][logstash.javapipeline    ][main] Pipeline terminated {"pipeline.id"=>"main"}
[2023-03-15T07:36:54.257+0000] {subprocess.py:93} INFO - [2023-03-15T07:36:54,257][INFO ][logstash.pipelinesregistry] Removed pipeline from registry successfully {:pipeline_id=>:main}
[2023-03-15T07:36:54.279+0000] {subprocess.py:93} INFO - [2023-03-15T07:36:54,279][INFO ][logstash.runner          ] Logstash shut down.
[2023-03-15T07:36:54.331+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-03-15T07:36:54.345+0000] {taskinstance.py:1318} INFO - Marking task as SUCCESS. dag_id=AUTOMATISATION, task_id=Chargement_des_données_in_ELK, execution_date=20230313T000400, start_date=20230315T073637, end_date=20230315T073654
[2023-03-15T07:36:54.366+0000] {local_task_job.py:208} INFO - Task exited with return code 0
[2023-03-15T07:36:54.379+0000] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
