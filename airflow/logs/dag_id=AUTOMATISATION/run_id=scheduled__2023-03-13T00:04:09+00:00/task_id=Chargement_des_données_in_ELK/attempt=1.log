[2023-03-13T23:03:47.586+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:04:09+00:00 [queued]>
[2023-03-13T23:03:47.598+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:04:09+00:00 [queued]>
[2023-03-13T23:03:47.598+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2023-03-13T23:03:47.598+0000] {taskinstance.py:1280} INFO - Starting attempt 1 of 4
[2023-03-13T23:03:47.598+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2023-03-13T23:03:47.626+0000] {taskinstance.py:1300} INFO - Executing <Task(BashOperator): Chargement_des_données_in_ELK> on 2023-03-13 00:04:09+00:00
[2023-03-13T23:03:47.629+0000] {standard_task_runner.py:55} INFO - Started process 81607 to run task
[2023-03-13T23:03:47.633+0000] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'AUTOMATISATION', 'Chargement_des_données_in_ELK', 'scheduled__2023-03-13T00:04:09+00:00', '--job-id', '445', '--raw', '--subdir', 'DAGS_FOLDER/main.py', '--cfg-path', '/tmp/tmp43gbkpfc']
[2023-03-13T23:03:47.635+0000] {standard_task_runner.py:83} INFO - Job 445: Subtask Chargement_des_données_in_ELK
[2023-03-13T23:03:47.705+0000] {task_command.py:388} INFO - Running <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:04:09+00:00 [running]> on host dev
[2023-03-13T23:03:47.795+0000] {taskinstance.py:1507} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=AUTOMATISATION
AIRFLOW_CTX_TASK_ID=Chargement_des_données_in_ELK
AIRFLOW_CTX_EXECUTION_DATE=2023-03-13T00:04:09+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-13T00:04:09+00:00
[2023-03-13T23:03:47.797+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-03-13T23:03:47.798+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'echo dev | sudo -S /usr/share/logstash/bin/logstash --path.settings=/etc/logstash/ -f /etc/logstash/conf.d/data.conf']
[2023-03-13T23:03:47.806+0000] {subprocess.py:86} INFO - Output:
[2023-03-13T23:03:47.898+0000] {subprocess.py:93} INFO - [sudo] Mot de passe de dev : Using bundled JDK: /usr/share/logstash/jdk
[2023-03-13T23:03:48.167+0000] {subprocess.py:93} INFO - OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
[2023-03-13T23:04:10.694+0000] {subprocess.py:93} INFO - Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties
[2023-03-13T23:04:10.807+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:10,803][INFO ][logstash.runner          ] Log4j configuration path used is: /etc/logstash/log4j2.properties
[2023-03-13T23:04:10.819+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:10,818][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.15.2", "jruby.version"=>"jruby 9.2.19.0 (2.5.8) 2021-06-15 55810c552b OpenJDK 64-Bit Server VM 11.0.12+7 on 11.0.12+7 +indy +jit [linux-x86_64]"}
[2023-03-13T23:04:11.210+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:11,210][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2023-03-13T23:04:13.017+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:13,015][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2023-03-13T23:04:13.857+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:13,856][INFO ][org.reflections.Reflections] Reflections took 141 ms to scan 1 urls, producing 120 keys and 417 values
[2023-03-13T23:04:15.231+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:15,231][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2023-03-13T23:04:15.611+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:15,609][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2023-03-13T23:04:15.797+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:15,797][WARN ][logstash.outputs.elasticsearch][main] Restored connection to ES instance {:url=>"http://localhost:9200/"}
[2023-03-13T23:04:15.854+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:15,854][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch version determined (7.15.2) {:es_version=>7}
[2023-03-13T23:04:15.857+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:15,857][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7}
[2023-03-13T23:04:16.035+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:16,034][INFO ][logstash.outputs.elasticsearch][main] Using a default mapping template {:es_version=>7, :ecs_compatibility=>:disabled}
[2023-03-13T23:04:16.100+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:16,100][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1000, "pipeline.sources"=>["/etc/logstash/conf.d/data.conf"], :thread=>"#<Thread:0x35ab7ae0 run>"}
[2023-03-13T23:04:17.248+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:17,247][INFO ][logstash.javapipeline    ][main] Pipeline Java execution initialization time {"seconds"=>1.14}
[2023-03-13T23:04:17.364+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:17,363][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2023-03-13T23:04:17.463+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:17,462][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2023-03-13T23:04:18.812+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:18,811][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.022328s) SELECT CAST(current_setting('server_version_num') AS integer) AS v
[2023-03-13T23:04:19.081+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:19,081][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.115236s) SELECT count(*) AS "count" FROM (SELECT * FROM customers) AS "t1" LIMIT 1
[2023-03-13T23:04:19.485+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:19,485][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.377511s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 0
[2023-03-13T23:04:25.878+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:25,877][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.300981s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 50000
[2023-03-13T23:04:28.568+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:28,567][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.129403s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 100000
[2023-03-13T23:04:31.070+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:31,069][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.106238s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 150000
[2023-03-13T23:04:33.938+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:33,937][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.143754s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 200000
[2023-03-13T23:04:36.636+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:36,636][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.109054s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 250000
[2023-03-13T23:04:39.044+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:39,044][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.143553s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 300000
[2023-03-13T23:04:41.441+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:41,441][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.110108s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 350000
[2023-03-13T23:04:44.492+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:44,492][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.141613s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 400000
[2023-03-13T23:04:46.891+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:46,891][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.152598s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 450000
[2023-03-13T23:04:49.177+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:49,177][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.128142s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 500000
[2023-03-13T23:04:51.578+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:51,578][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.129784s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 550000
[2023-03-13T23:04:54.105+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:54,105][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.126356s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 600000
[2023-03-13T23:04:56.426+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:56,425][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.142472s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 650000
[2023-03-13T23:04:58.753+0000] {subprocess.py:93} INFO - [2023-03-13T23:04:58,752][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.135426s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 700000
[2023-03-13T23:05:01.101+0000] {subprocess.py:93} INFO - [2023-03-13T23:05:01,101][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.145120s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 750000
[2023-03-13T23:05:03.778+0000] {subprocess.py:93} INFO - [2023-03-13T23:05:03,778][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.153482s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 800000
[2023-03-13T23:05:06.396+0000] {subprocess.py:93} INFO - [2023-03-13T23:05:06,396][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.171880s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 850000
[2023-03-13T23:05:08.742+0000] {subprocess.py:93} INFO - [2023-03-13T23:05:08,742][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.175871s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 900000
[2023-03-13T23:05:15.473+0000] {subprocess.py:93} INFO - [2023-03-13T23:05:15,473][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.148845s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 950000
[2023-03-13T23:05:18.984+0000] {subprocess.py:93} INFO - [2023-03-13T23:05:18,984][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.170842s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 1000000
[2023-03-13T23:05:21.217+0000] {subprocess.py:93} INFO - [2023-03-13T23:05:21,217][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.162027s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 1050000
[2023-03-13T23:05:23.679+0000] {subprocess.py:93} INFO - [2023-03-13T23:05:23,679][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.156076s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 1100000
[2023-03-13T23:05:25.847+0000] {subprocess.py:93} INFO - [2023-03-13T23:05:25,847][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.181279s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 1150000
[2023-03-13T23:05:28.038+0000] {subprocess.py:93} INFO - [2023-03-13T23:05:28,038][INFO ][logstash.javapipeline    ][main] Pipeline terminated {"pipeline.id"=>"main"}
[2023-03-13T23:05:28.292+0000] {subprocess.py:93} INFO - [2023-03-13T23:05:28,291][INFO ][logstash.pipelinesregistry] Removed pipeline from registry successfully {:pipeline_id=>:main}
[2023-03-13T23:05:28.344+0000] {subprocess.py:93} INFO - [2023-03-13T23:05:28,344][INFO ][logstash.runner          ] Logstash shut down.
[2023-03-13T23:05:28.523+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-03-13T23:05:28.557+0000] {taskinstance.py:1318} INFO - Marking task as SUCCESS. dag_id=AUTOMATISATION, task_id=Chargement_des_données_in_ELK, execution_date=20230313T000409, start_date=20230313T230347, end_date=20230313T230528
[2023-03-13T23:05:28.597+0000] {local_task_job.py:208} INFO - Task exited with return code 0
[2023-03-13T23:05:28.611+0000] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-03-15T07:37:38.225+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:04:09+00:00 [queued]>
[2023-03-15T07:37:38.229+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:04:09+00:00 [queued]>
[2023-03-15T07:37:38.229+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2023-03-15T07:37:38.230+0000] {taskinstance.py:1280} INFO - Starting attempt 1 of 4
[2023-03-15T07:37:38.230+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2023-03-15T07:37:38.240+0000] {taskinstance.py:1300} INFO - Executing <Task(BashOperator): Chargement_des_données_in_ELK> on 2023-03-13 00:04:09+00:00
[2023-03-15T07:37:38.242+0000] {standard_task_runner.py:55} INFO - Started process 134540 to run task
[2023-03-15T07:37:38.243+0000] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'AUTOMATISATION', 'Chargement_des_données_in_ELK', 'scheduled__2023-03-13T00:04:09+00:00', '--job-id', '815', '--raw', '--subdir', 'DAGS_FOLDER/main.py', '--cfg-path', '/tmp/tmph0zhhya_']
[2023-03-15T07:37:38.244+0000] {standard_task_runner.py:83} INFO - Job 815: Subtask Chargement_des_données_in_ELK
[2023-03-15T07:37:38.272+0000] {task_command.py:388} INFO - Running <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:04:09+00:00 [running]> on host dev
[2023-03-15T07:37:38.309+0000] {taskinstance.py:1507} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=AUTOMATISATION
AIRFLOW_CTX_TASK_ID=Chargement_des_données_in_ELK
AIRFLOW_CTX_EXECUTION_DATE=2023-03-13T00:04:09+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-13T00:04:09+00:00
[2023-03-15T07:37:38.309+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-03-15T07:37:38.310+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'echo dev | sudo -S /usr/share/logstash/bin/logstash --path.settings=/etc/logstash/ -f /etc/logstash/conf.d/data.conf']
[2023-03-15T07:37:38.314+0000] {subprocess.py:86} INFO - Output:
[2023-03-15T07:37:38.349+0000] {subprocess.py:93} INFO - [sudo] Mot de passe de dev : Using bundled JDK: /usr/share/logstash/jdk
[2023-03-15T07:37:38.469+0000] {subprocess.py:93} INFO - OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
[2023-03-15T07:37:50.453+0000] {subprocess.py:93} INFO - Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties
[2023-03-15T07:37:50.533+0000] {subprocess.py:93} INFO - [2023-03-15T07:37:50,531][INFO ][logstash.runner          ] Log4j configuration path used is: /etc/logstash/log4j2.properties
[2023-03-15T07:37:50.541+0000] {subprocess.py:93} INFO - [2023-03-15T07:37:50,541][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.15.2", "jruby.version"=>"jruby 9.2.19.0 (2.5.8) 2021-06-15 55810c552b OpenJDK 64-Bit Server VM 11.0.12+7 on 11.0.12+7 +indy +jit [linux-x86_64]"}
[2023-03-15T07:37:50.826+0000] {subprocess.py:93} INFO - [2023-03-15T07:37:50,826][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2023-03-15T07:37:51.665+0000] {subprocess.py:93} INFO - [2023-03-15T07:37:51,664][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2023-03-15T07:37:52.022+0000] {subprocess.py:93} INFO - [2023-03-15T07:37:52,021][INFO ][org.reflections.Reflections] Reflections took 58 ms to scan 1 urls, producing 120 keys and 417 values
[2023-03-15T07:37:52.768+0000] {subprocess.py:93} INFO - [2023-03-15T07:37:52,767][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2023-03-15T07:37:53.006+0000] {subprocess.py:93} INFO - [2023-03-15T07:37:53,005][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2023-03-15T07:37:53.131+0000] {subprocess.py:93} INFO - [2023-03-15T07:37:53,131][WARN ][logstash.outputs.elasticsearch][main] Restored connection to ES instance {:url=>"http://localhost:9200/"}
[2023-03-15T07:37:53.159+0000] {subprocess.py:93} INFO - [2023-03-15T07:37:53,159][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch version determined (7.15.2) {:es_version=>7}
[2023-03-15T07:37:53.161+0000] {subprocess.py:93} INFO - [2023-03-15T07:37:53,161][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7}
[2023-03-15T07:37:53.255+0000] {subprocess.py:93} INFO - [2023-03-15T07:37:53,255][INFO ][logstash.outputs.elasticsearch][main] Using a default mapping template {:es_version=>7, :ecs_compatibility=>:disabled}
[2023-03-15T07:37:53.281+0000] {subprocess.py:93} INFO - [2023-03-15T07:37:53,280][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1000, "pipeline.sources"=>["/etc/logstash/conf.d/data.conf"], :thread=>"#<Thread:0x5e91e787 run>"}
[2023-03-15T07:37:53.821+0000] {subprocess.py:93} INFO - [2023-03-15T07:37:53,820][INFO ][logstash.javapipeline    ][main] Pipeline Java execution initialization time {"seconds"=>0.54}
[2023-03-15T07:37:53.890+0000] {subprocess.py:93} INFO - [2023-03-15T07:37:53,889][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2023-03-15T07:37:53.959+0000] {subprocess.py:93} INFO - [2023-03-15T07:37:53,959][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2023-03-15T07:37:54.712+0000] {subprocess.py:93} INFO - [2023-03-15T07:37:54,712][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.014605s) SELECT CAST(current_setting('server_version_num') AS integer) AS v
[2023-03-15T07:37:54.806+0000] {subprocess.py:93} INFO - [2023-03-15T07:37:54,805][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.001409s) SELECT count(*) AS "count" FROM (SELECT * FROM customers) AS "t1" LIMIT 1
[2023-03-15T07:37:55.218+0000] {subprocess.py:93} INFO - [2023-03-15T07:37:55,218][INFO ][logstash.javapipeline    ][main] Pipeline terminated {"pipeline.id"=>"main"}
[2023-03-15T07:37:55.510+0000] {subprocess.py:93} INFO - [2023-03-15T07:37:55,510][INFO ][logstash.pipelinesregistry] Removed pipeline from registry successfully {:pipeline_id=>:main}
[2023-03-15T07:37:55.532+0000] {subprocess.py:93} INFO - [2023-03-15T07:37:55,532][INFO ][logstash.runner          ] Logstash shut down.
[2023-03-15T07:37:55.586+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-03-15T07:37:55.600+0000] {taskinstance.py:1318} INFO - Marking task as SUCCESS. dag_id=AUTOMATISATION, task_id=Chargement_des_données_in_ELK, execution_date=20230313T000409, start_date=20230315T073738, end_date=20230315T073755
[2023-03-15T07:37:55.642+0000] {local_task_job.py:208} INFO - Task exited with return code 0
[2023-03-15T07:37:55.656+0000] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
