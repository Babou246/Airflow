[2023-03-13T23:48:36.173+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:05:12+00:00 [queued]>
[2023-03-13T23:48:36.185+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:05:12+00:00 [queued]>
[2023-03-13T23:48:36.185+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2023-03-13T23:48:36.185+0000] {taskinstance.py:1280} INFO - Starting attempt 1 of 4
[2023-03-13T23:48:36.185+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2023-03-13T23:48:36.214+0000] {taskinstance.py:1300} INFO - Executing <Task(BashOperator): Chargement_des_données_in_ELK> on 2023-03-13 00:05:12+00:00
[2023-03-13T23:48:36.217+0000] {standard_task_runner.py:55} INFO - Started process 90681 to run task
[2023-03-13T23:48:36.221+0000] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'AUTOMATISATION', 'Chargement_des_données_in_ELK', 'scheduled__2023-03-13T00:05:12+00:00', '--job-id', '487', '--raw', '--subdir', 'DAGS_FOLDER/main.py', '--cfg-path', '/tmp/tmpky93s3mh']
[2023-03-13T23:48:36.223+0000] {standard_task_runner.py:83} INFO - Job 487: Subtask Chargement_des_données_in_ELK
[2023-03-13T23:48:36.292+0000] {task_command.py:388} INFO - Running <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:05:12+00:00 [running]> on host dev
[2023-03-13T23:48:36.378+0000] {taskinstance.py:1507} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=AUTOMATISATION
AIRFLOW_CTX_TASK_ID=Chargement_des_données_in_ELK
AIRFLOW_CTX_EXECUTION_DATE=2023-03-13T00:05:12+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-13T00:05:12+00:00
[2023-03-13T23:48:36.380+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-03-13T23:48:36.381+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'echo dev | sudo -S /usr/share/logstash/bin/logstash --path.settings=/etc/logstash/ -f /etc/logstash/conf.d/data.conf']
[2023-03-13T23:48:36.390+0000] {subprocess.py:86} INFO - Output:
[2023-03-13T23:48:36.476+0000] {subprocess.py:93} INFO - [sudo] Mot de passe de dev : Using bundled JDK: /usr/share/logstash/jdk
[2023-03-13T23:48:36.729+0000] {subprocess.py:93} INFO - OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
[2023-03-13T23:48:57.905+0000] {subprocess.py:93} INFO - Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties
[2023-03-13T23:48:58.013+0000] {subprocess.py:93} INFO - [2023-03-13T23:48:58,010][INFO ][logstash.runner          ] Log4j configuration path used is: /etc/logstash/log4j2.properties
[2023-03-13T23:48:58.024+0000] {subprocess.py:93} INFO - [2023-03-13T23:48:58,023][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.15.2", "jruby.version"=>"jruby 9.2.19.0 (2.5.8) 2021-06-15 55810c552b OpenJDK 64-Bit Server VM 11.0.12+7 on 11.0.12+7 +indy +jit [linux-x86_64]"}
[2023-03-13T23:48:58.446+0000] {subprocess.py:93} INFO - [2023-03-13T23:48:58,445][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2023-03-13T23:49:00.105+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:00,103][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2023-03-13T23:49:00.741+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:00,740][INFO ][org.reflections.Reflections] Reflections took 92 ms to scan 1 urls, producing 120 keys and 417 values
[2023-03-13T23:49:02.259+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:02,259][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2023-03-13T23:49:02.654+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:02,652][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2023-03-13T23:49:02.841+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:02,840][WARN ][logstash.outputs.elasticsearch][main] Restored connection to ES instance {:url=>"http://localhost:9200/"}
[2023-03-13T23:49:02.913+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:02,913][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch version determined (7.15.2) {:es_version=>7}
[2023-03-13T23:49:02.918+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:02,917][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7}
[2023-03-13T23:49:03.142+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:03,142][INFO ][logstash.outputs.elasticsearch][main] Using a default mapping template {:es_version=>7, :ecs_compatibility=>:disabled}
[2023-03-13T23:49:03.197+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:03,196][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1000, "pipeline.sources"=>["/etc/logstash/conf.d/data.conf"], :thread=>"#<Thread:0xe2a9b62 run>"}
[2023-03-13T23:49:04.327+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:04,327][INFO ][logstash.javapipeline    ][main] Pipeline Java execution initialization time {"seconds"=>1.13}
[2023-03-13T23:49:04.475+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:04,475][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2023-03-13T23:49:04.619+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:04,609][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2023-03-13T23:49:06.130+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:06,129][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.025456s) SELECT CAST(current_setting('server_version_num') AS integer) AS v
[2023-03-13T23:49:06.440+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:06,439][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.115414s) SELECT count(*) AS "count" FROM (SELECT * FROM customers) AS "t1" LIMIT 1
[2023-03-13T23:49:06.945+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:06,945][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.460235s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 0
[2023-03-13T23:49:14.824+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:14,824][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.375497s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 50000
[2023-03-13T23:49:19.878+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:19,877][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.385559s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 100000
[2023-03-13T23:49:22.761+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:22,760][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.135557s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 150000
[2023-03-13T23:49:26.724+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:26,724][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.116690s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 200000
[2023-03-13T23:49:36.187+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:36,187][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.139156s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 250000
[2023-03-13T23:49:39.365+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:39,365][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.129869s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 300000
[2023-03-13T23:49:42.229+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:42,229][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.157981s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 350000
[2023-03-13T23:49:44.699+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:44,699][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.128252s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 400000
[2023-03-13T23:49:47.601+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:47,600][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.154281s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 450000
[2023-03-13T23:49:50.462+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:50,462][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.141020s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 500000
[2023-03-13T23:49:52.936+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:52,936][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.140365s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 550000
[2023-03-13T23:49:55.261+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:55,261][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.173558s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 600000
[2023-03-13T23:49:57.788+0000] {subprocess.py:93} INFO - [2023-03-13T23:49:57,788][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.137582s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 650000
[2023-03-13T23:50:00.406+0000] {subprocess.py:93} INFO - [2023-03-13T23:50:00,405][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.141990s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 700000
[2023-03-13T23:50:03.108+0000] {subprocess.py:93} INFO - [2023-03-13T23:50:03,107][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.145417s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 750000
[2023-03-13T23:50:05.297+0000] {subprocess.py:93} INFO - [2023-03-13T23:50:05,297][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.142434s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 800000
[2023-03-13T23:50:08.671+0000] {subprocess.py:93} INFO - [2023-03-13T23:50:08,670][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.167920s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 850000
[2023-03-13T23:50:11.811+0000] {subprocess.py:93} INFO - [2023-03-13T23:50:11,811][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.183068s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 900000
[2023-03-13T23:50:14.386+0000] {subprocess.py:93} INFO - [2023-03-13T23:50:14,385][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.182539s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 950000
[2023-03-13T23:50:16.736+0000] {subprocess.py:93} INFO - [2023-03-13T23:50:16,736][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.205733s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 1000000
[2023-03-13T23:50:19.249+0000] {subprocess.py:93} INFO - [2023-03-13T23:50:19,249][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.179661s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 1050000
[2023-03-13T23:50:22.037+0000] {subprocess.py:93} INFO - [2023-03-13T23:50:22,036][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.168423s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 1100000
[2023-03-13T23:50:24.548+0000] {subprocess.py:93} INFO - [2023-03-13T23:50:24,547][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.164575s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 1150000
[2023-03-13T23:50:27.170+0000] {subprocess.py:93} INFO - [2023-03-13T23:50:27,169][INFO ][logstash.javapipeline    ][main] Pipeline terminated {"pipeline.id"=>"main"}
[2023-03-13T23:50:27.495+0000] {subprocess.py:93} INFO - [2023-03-13T23:50:27,494][INFO ][logstash.pipelinesregistry] Removed pipeline from registry successfully {:pipeline_id=>:main}
[2023-03-13T23:50:27.548+0000] {subprocess.py:93} INFO - [2023-03-13T23:50:27,548][INFO ][logstash.runner          ] Logstash shut down.
[2023-03-13T23:50:27.736+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-03-13T23:50:27.770+0000] {taskinstance.py:1318} INFO - Marking task as SUCCESS. dag_id=AUTOMATISATION, task_id=Chargement_des_données_in_ELK, execution_date=20230313T000512, start_date=20230313T234836, end_date=20230313T235027
[2023-03-13T23:50:27.799+0000] {local_task_job.py:208} INFO - Task exited with return code 0
[2023-03-13T23:50:27.814+0000] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-03-15T07:44:44.949+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:05:12+00:00 [queued]>
[2023-03-15T07:44:44.956+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:05:12+00:00 [queued]>
[2023-03-15T07:44:44.956+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2023-03-15T07:44:44.956+0000] {taskinstance.py:1280} INFO - Starting attempt 1 of 4
[2023-03-15T07:44:44.956+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2023-03-15T07:44:44.972+0000] {taskinstance.py:1300} INFO - Executing <Task(BashOperator): Chargement_des_données_in_ELK> on 2023-03-13 00:05:12+00:00
[2023-03-15T07:44:44.974+0000] {standard_task_runner.py:55} INFO - Started process 138841 to run task
[2023-03-15T07:44:44.976+0000] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'AUTOMATISATION', 'Chargement_des_données_in_ELK', 'scheduled__2023-03-13T00:05:12+00:00', '--job-id', '857', '--raw', '--subdir', 'DAGS_FOLDER/main.py', '--cfg-path', '/tmp/tmpium0jnn6']
[2023-03-15T07:44:44.977+0000] {standard_task_runner.py:83} INFO - Job 857: Subtask Chargement_des_données_in_ELK
[2023-03-15T07:44:45.011+0000] {task_command.py:388} INFO - Running <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:05:12+00:00 [running]> on host dev
[2023-03-15T07:44:45.055+0000] {taskinstance.py:1507} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=AUTOMATISATION
AIRFLOW_CTX_TASK_ID=Chargement_des_données_in_ELK
AIRFLOW_CTX_EXECUTION_DATE=2023-03-13T00:05:12+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-13T00:05:12+00:00
[2023-03-15T07:44:45.056+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-03-15T07:44:45.056+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'echo dev | sudo -S /usr/share/logstash/bin/logstash --path.settings=/etc/logstash/ -f /etc/logstash/conf.d/data.conf']
[2023-03-15T07:44:45.061+0000] {subprocess.py:86} INFO - Output:
[2023-03-15T07:44:45.102+0000] {subprocess.py:93} INFO - [sudo] Mot de passe de dev : Using bundled JDK: /usr/share/logstash/jdk
[2023-03-15T07:44:45.236+0000] {subprocess.py:93} INFO - OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
[2023-03-15T07:44:59.108+0000] {subprocess.py:93} INFO - Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties
[2023-03-15T07:44:59.171+0000] {subprocess.py:93} INFO - [2023-03-15T07:44:59,168][INFO ][logstash.runner          ] Log4j configuration path used is: /etc/logstash/log4j2.properties
[2023-03-15T07:44:59.176+0000] {subprocess.py:93} INFO - [2023-03-15T07:44:59,176][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.15.2", "jruby.version"=>"jruby 9.2.19.0 (2.5.8) 2021-06-15 55810c552b OpenJDK 64-Bit Server VM 11.0.12+7 on 11.0.12+7 +indy +jit [linux-x86_64]"}
[2023-03-15T07:44:59.440+0000] {subprocess.py:93} INFO - [2023-03-15T07:44:59,440][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2023-03-15T07:45:00.397+0000] {subprocess.py:93} INFO - [2023-03-15T07:45:00,396][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2023-03-15T07:45:00.844+0000] {subprocess.py:93} INFO - [2023-03-15T07:45:00,844][INFO ][org.reflections.Reflections] Reflections took 80 ms to scan 1 urls, producing 120 keys and 417 values
[2023-03-15T07:45:01.679+0000] {subprocess.py:93} INFO - [2023-03-15T07:45:01,678][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2023-03-15T07:45:02.066+0000] {subprocess.py:93} INFO - [2023-03-15T07:45:02,064][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2023-03-15T07:45:02.228+0000] {subprocess.py:93} INFO - [2023-03-15T07:45:02,228][WARN ][logstash.outputs.elasticsearch][main] Restored connection to ES instance {:url=>"http://localhost:9200/"}
[2023-03-15T07:45:02.278+0000] {subprocess.py:93} INFO - [2023-03-15T07:45:02,277][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch version determined (7.15.2) {:es_version=>7}
[2023-03-15T07:45:02.281+0000] {subprocess.py:93} INFO - [2023-03-15T07:45:02,280][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7}
[2023-03-15T07:45:02.424+0000] {subprocess.py:93} INFO - [2023-03-15T07:45:02,423][INFO ][logstash.outputs.elasticsearch][main] Using a default mapping template {:es_version=>7, :ecs_compatibility=>:disabled}
[2023-03-15T07:45:02.446+0000] {subprocess.py:93} INFO - [2023-03-15T07:45:02,445][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1000, "pipeline.sources"=>["/etc/logstash/conf.d/data.conf"], :thread=>"#<Thread:0x6076fc3b run>"}
[2023-03-15T07:45:03.052+0000] {subprocess.py:93} INFO - [2023-03-15T07:45:03,051][INFO ][logstash.javapipeline    ][main] Pipeline Java execution initialization time {"seconds"=>0.6}
[2023-03-15T07:45:03.126+0000] {subprocess.py:93} INFO - [2023-03-15T07:45:03,125][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2023-03-15T07:45:03.210+0000] {subprocess.py:93} INFO - [2023-03-15T07:45:03,209][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2023-03-15T07:45:04.195+0000] {subprocess.py:93} INFO - [2023-03-15T07:45:04,195][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.014907s) SELECT CAST(current_setting('server_version_num') AS integer) AS v
[2023-03-15T07:45:04.308+0000] {subprocess.py:93} INFO - [2023-03-15T07:45:04,308][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.006754s) SELECT count(*) AS "count" FROM (SELECT * FROM customers) AS "t1" LIMIT 1
[2023-03-15T07:45:04.575+0000] {subprocess.py:93} INFO - [2023-03-15T07:45:04,574][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.241586s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 0
[2023-03-15T07:45:08.352+0000] {subprocess.py:93} INFO - [2023-03-15T07:45:08,352][INFO ][logstash.javapipeline    ][main] Pipeline terminated {"pipeline.id"=>"main"}
[2023-03-15T07:45:08.773+0000] {subprocess.py:93} INFO - [2023-03-15T07:45:08,773][INFO ][logstash.pipelinesregistry] Removed pipeline from registry successfully {:pipeline_id=>:main}
[2023-03-15T07:45:08.949+0000] {subprocess.py:93} INFO - [2023-03-15T07:45:08,949][INFO ][logstash.runner          ] Logstash shut down.
[2023-03-15T07:45:09.541+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-03-15T07:45:09.697+0000] {taskinstance.py:1318} INFO - Marking task as SUCCESS. dag_id=AUTOMATISATION, task_id=Chargement_des_données_in_ELK, execution_date=20230313T000512, start_date=20230315T074444, end_date=20230315T074509
[2023-03-15T07:45:09.920+0000] {local_task_job.py:208} INFO - Task exited with return code 0
[2023-03-15T07:45:09.926+0000] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
