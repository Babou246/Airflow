[2023-03-14T00:03:57.510+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:05:33+00:00 [queued]>
[2023-03-14T00:03:57.521+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:05:33+00:00 [queued]>
[2023-03-14T00:03:57.521+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2023-03-14T00:03:57.521+0000] {taskinstance.py:1280} INFO - Starting attempt 1 of 4
[2023-03-14T00:03:57.521+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2023-03-14T00:03:57.546+0000] {taskinstance.py:1300} INFO - Executing <Task(BashOperator): Chargement_des_données_in_ELK> on 2023-03-13 00:05:33+00:00
[2023-03-14T00:03:57.549+0000] {standard_task_runner.py:55} INFO - Started process 94095 to run task
[2023-03-14T00:03:57.553+0000] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'AUTOMATISATION', 'Chargement_des_données_in_ELK', 'scheduled__2023-03-13T00:05:33+00:00', '--job-id', '501', '--raw', '--subdir', 'DAGS_FOLDER/main.py', '--cfg-path', '/tmp/tmp2i3a0vyv']
[2023-03-14T00:03:57.555+0000] {standard_task_runner.py:83} INFO - Job 501: Subtask Chargement_des_données_in_ELK
[2023-03-14T00:03:57.621+0000] {task_command.py:388} INFO - Running <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:05:33+00:00 [running]> on host dev
[2023-03-14T00:03:57.706+0000] {taskinstance.py:1507} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=AUTOMATISATION
AIRFLOW_CTX_TASK_ID=Chargement_des_données_in_ELK
AIRFLOW_CTX_EXECUTION_DATE=2023-03-13T00:05:33+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-13T00:05:33+00:00
[2023-03-14T00:03:57.708+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-03-14T00:03:57.708+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'echo dev | sudo -S /usr/share/logstash/bin/logstash --path.settings=/etc/logstash/ -f /etc/logstash/conf.d/data.conf']
[2023-03-14T00:03:57.716+0000] {subprocess.py:86} INFO - Output:
[2023-03-14T00:03:57.800+0000] {subprocess.py:93} INFO - [sudo] Mot de passe de dev : Using bundled JDK: /usr/share/logstash/jdk
[2023-03-14T00:03:58.064+0000] {subprocess.py:93} INFO - OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
[2023-03-14T00:04:21.334+0000] {subprocess.py:93} INFO - Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties
[2023-03-14T00:04:21.464+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:21,460][INFO ][logstash.runner          ] Log4j configuration path used is: /etc/logstash/log4j2.properties
[2023-03-14T00:04:21.478+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:21,477][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.15.2", "jruby.version"=>"jruby 9.2.19.0 (2.5.8) 2021-06-15 55810c552b OpenJDK 64-Bit Server VM 11.0.12+7 on 11.0.12+7 +indy +jit [linux-x86_64]"}
[2023-03-14T00:04:21.904+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:21,904][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2023-03-14T00:04:23.927+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:23,926][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2023-03-14T00:04:24.753+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:24,752][INFO ][org.reflections.Reflections] Reflections took 156 ms to scan 1 urls, producing 120 keys and 417 values
[2023-03-14T00:04:26.346+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:26,345][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2023-03-14T00:04:26.803+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:26,800][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2023-03-14T00:04:26.999+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:26,998][WARN ][logstash.outputs.elasticsearch][main] Restored connection to ES instance {:url=>"http://localhost:9200/"}
[2023-03-14T00:04:27.057+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:27,056][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch version determined (7.15.2) {:es_version=>7}
[2023-03-14T00:04:27.060+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:27,060][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7}
[2023-03-14T00:04:27.259+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:27,258][INFO ][logstash.outputs.elasticsearch][main] Using a default mapping template {:es_version=>7, :ecs_compatibility=>:disabled}
[2023-03-14T00:04:27.290+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:27,290][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1000, "pipeline.sources"=>["/etc/logstash/conf.d/data.conf"], :thread=>"#<Thread:0x6d10efb9 run>"}
[2023-03-14T00:04:28.583+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:28,582][INFO ][logstash.javapipeline    ][main] Pipeline Java execution initialization time {"seconds"=>1.29}
[2023-03-14T00:04:28.718+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:28,718][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2023-03-14T00:04:28.842+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:28,841][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2023-03-14T00:04:30.462+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:30,461][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.025496s) SELECT CAST(current_setting('server_version_num') AS integer) AS v
[2023-03-14T00:04:30.737+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:30,737][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.103919s) SELECT count(*) AS "count" FROM (SELECT * FROM customers) AS "t1" LIMIT 1
[2023-03-14T00:04:31.173+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:31,173][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.409264s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 0
[2023-03-14T00:04:39.465+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:39,465][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.404999s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 50000
[2023-03-14T00:04:42.507+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:42,507][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.172479s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 100000
[2023-03-14T00:04:45.724+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:45,724][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.128025s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 150000
[2023-03-14T00:04:48.617+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:48,616][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.109501s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 200000
[2023-03-14T00:04:51.925+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:51,925][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.242907s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 250000
[2023-03-14T00:04:55.454+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:55,454][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.129279s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 300000
[2023-03-14T00:04:58.649+0000] {subprocess.py:93} INFO - [2023-03-14T00:04:58,649][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.159600s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 350000
[2023-03-14T00:05:03.378+0000] {subprocess.py:93} INFO - [2023-03-14T00:05:03,377][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.129565s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 400000
[2023-03-14T00:05:05.576+0000] {subprocess.py:93} INFO - [2023-03-14T00:05:05,575][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.116585s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 450000
[2023-03-14T00:05:08.103+0000] {subprocess.py:93} INFO - [2023-03-14T00:05:08,103][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.134835s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 500000
[2023-03-14T00:05:10.824+0000] {subprocess.py:93} INFO - [2023-03-14T00:05:10,824][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.211222s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 550000
[2023-03-14T00:05:14.881+0000] {subprocess.py:93} INFO - [2023-03-14T00:05:14,881][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.163841s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 600000
[2023-03-14T00:05:17.488+0000] {subprocess.py:93} INFO - [2023-03-14T00:05:17,488][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.301359s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 650000
[2023-03-14T00:05:19.948+0000] {subprocess.py:93} INFO - [2023-03-14T00:05:19,947][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.149490s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 700000
[2023-03-14T00:05:22.288+0000] {subprocess.py:93} INFO - [2023-03-14T00:05:22,288][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.138538s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 750000
[2023-03-14T00:05:27.482+0000] {subprocess.py:93} INFO - [2023-03-14T00:05:27,482][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.150817s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 800000
[2023-03-14T00:05:30.158+0000] {subprocess.py:93} INFO - [2023-03-14T00:05:30,158][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.164443s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 850000
[2023-03-14T00:05:32.627+0000] {subprocess.py:93} INFO - [2023-03-14T00:05:32,627][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.145874s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 900000
[2023-03-14T00:05:34.956+0000] {subprocess.py:93} INFO - [2023-03-14T00:05:34,956][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.184123s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 950000
[2023-03-14T00:05:37.352+0000] {subprocess.py:93} INFO - [2023-03-14T00:05:37,352][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.200698s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 1000000
[2023-03-14T00:05:39.753+0000] {subprocess.py:93} INFO - [2023-03-14T00:05:39,752][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.168743s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 1050000
[2023-03-14T00:05:42.278+0000] {subprocess.py:93} INFO - [2023-03-14T00:05:42,277][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.161573s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 1100000
[2023-03-14T00:05:44.574+0000] {subprocess.py:93} INFO - [2023-03-14T00:05:44,574][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.171040s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 1150000
[2023-03-14T00:05:47.227+0000] {subprocess.py:93} INFO - [2023-03-14T00:05:47,227][INFO ][logstash.javapipeline    ][main] Pipeline terminated {"pipeline.id"=>"main"}
[2023-03-14T00:05:47.689+0000] {subprocess.py:93} INFO - [2023-03-14T00:05:47,688][INFO ][logstash.pipelinesregistry] Removed pipeline from registry successfully {:pipeline_id=>:main}
[2023-03-14T00:05:47.735+0000] {subprocess.py:93} INFO - [2023-03-14T00:05:47,735][INFO ][logstash.runner          ] Logstash shut down.
[2023-03-14T00:05:47.909+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-03-14T00:05:47.945+0000] {taskinstance.py:1318} INFO - Marking task as SUCCESS. dag_id=AUTOMATISATION, task_id=Chargement_des_données_in_ELK, execution_date=20230313T000533, start_date=20230314T000357, end_date=20230314T000547
[2023-03-14T00:05:47.971+0000] {local_task_job.py:208} INFO - Task exited with return code 0
[2023-03-14T00:05:47.985+0000] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-03-15T07:49:15.500+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:05:33+00:00 [queued]>
[2023-03-15T07:49:15.506+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:05:33+00:00 [queued]>
[2023-03-15T07:49:15.506+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2023-03-15T07:49:15.506+0000] {taskinstance.py:1280} INFO - Starting attempt 1 of 4
[2023-03-15T07:49:15.506+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2023-03-15T07:49:15.519+0000] {taskinstance.py:1300} INFO - Executing <Task(BashOperator): Chargement_des_données_in_ELK> on 2023-03-13 00:05:33+00:00
[2023-03-15T07:49:15.522+0000] {standard_task_runner.py:55} INFO - Started process 141492 to run task
[2023-03-15T07:49:15.524+0000] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'AUTOMATISATION', 'Chargement_des_données_in_ELK', 'scheduled__2023-03-13T00:05:33+00:00', '--job-id', '872', '--raw', '--subdir', 'DAGS_FOLDER/main.py', '--cfg-path', '/tmp/tmpykpdjppf']
[2023-03-15T07:49:15.525+0000] {standard_task_runner.py:83} INFO - Job 872: Subtask Chargement_des_données_in_ELK
[2023-03-15T07:49:15.561+0000] {task_command.py:388} INFO - Running <TaskInstance: AUTOMATISATION.Chargement_des_données_in_ELK scheduled__2023-03-13T00:05:33+00:00 [running]> on host dev
[2023-03-15T07:49:15.619+0000] {taskinstance.py:1507} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=AUTOMATISATION
AIRFLOW_CTX_TASK_ID=Chargement_des_données_in_ELK
AIRFLOW_CTX_EXECUTION_DATE=2023-03-13T00:05:33+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-03-13T00:05:33+00:00
[2023-03-15T07:49:15.620+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-03-15T07:49:15.621+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'echo dev | sudo -S /usr/share/logstash/bin/logstash --path.settings=/etc/logstash/ -f /etc/logstash/conf.d/data.conf']
[2023-03-15T07:49:15.627+0000] {subprocess.py:86} INFO - Output:
[2023-03-15T07:49:15.672+0000] {subprocess.py:93} INFO - [sudo] Mot de passe de dev : Using bundled JDK: /usr/share/logstash/jdk
[2023-03-15T07:49:15.829+0000] {subprocess.py:93} INFO - OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.
[2023-03-15T07:49:32.547+0000] {subprocess.py:93} INFO - Sending Logstash logs to /var/log/logstash which is now configured via log4j2.properties
[2023-03-15T07:49:32.617+0000] {subprocess.py:93} INFO - [2023-03-15T07:49:32,615][INFO ][logstash.runner          ] Log4j configuration path used is: /etc/logstash/log4j2.properties
[2023-03-15T07:49:32.625+0000] {subprocess.py:93} INFO - [2023-03-15T07:49:32,624][INFO ][logstash.runner          ] Starting Logstash {"logstash.version"=>"7.15.2", "jruby.version"=>"jruby 9.2.19.0 (2.5.8) 2021-06-15 55810c552b OpenJDK 64-Bit Server VM 11.0.12+7 on 11.0.12+7 +indy +jit [linux-x86_64]"}
[2023-03-15T07:49:32.957+0000] {subprocess.py:93} INFO - [2023-03-15T07:49:32,956][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified
[2023-03-15T07:49:34.483+0000] {subprocess.py:93} INFO - [2023-03-15T07:49:34,482][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
[2023-03-15T07:49:35.309+0000] {subprocess.py:93} INFO - [2023-03-15T07:49:35,309][INFO ][org.reflections.Reflections] Reflections took 127 ms to scan 1 urls, producing 120 keys and 417 values
[2023-03-15T07:49:36.335+0000] {subprocess.py:93} INFO - [2023-03-15T07:49:36,334][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>"LogStash::Outputs::ElasticSearch", :hosts=>["//localhost:9200"]}
[2023-03-15T07:49:36.621+0000] {subprocess.py:93} INFO - [2023-03-15T07:49:36,619][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}}
[2023-03-15T07:49:36.757+0000] {subprocess.py:93} INFO - [2023-03-15T07:49:36,757][WARN ][logstash.outputs.elasticsearch][main] Restored connection to ES instance {:url=>"http://localhost:9200/"}
[2023-03-15T07:49:36.803+0000] {subprocess.py:93} INFO - [2023-03-15T07:49:36,803][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch version determined (7.15.2) {:es_version=>7}
[2023-03-15T07:49:36.806+0000] {subprocess.py:93} INFO - [2023-03-15T07:49:36,806][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7}
[2023-03-15T07:49:37.005+0000] {subprocess.py:93} INFO - [2023-03-15T07:49:37,004][INFO ][logstash.outputs.elasticsearch][main] Using a default mapping template {:es_version=>7, :ecs_compatibility=>:disabled}
[2023-03-15T07:49:37.034+0000] {subprocess.py:93} INFO - [2023-03-15T07:49:37,033][INFO ][logstash.javapipeline    ][main] Starting pipeline {:pipeline_id=>"main", "pipeline.workers"=>8, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>50, "pipeline.max_inflight"=>1000, "pipeline.sources"=>["/etc/logstash/conf.d/data.conf"], :thread=>"#<Thread:0x18a515b7 run>"}
[2023-03-15T07:49:37.770+0000] {subprocess.py:93} INFO - [2023-03-15T07:49:37,769][INFO ][logstash.javapipeline    ][main] Pipeline Java execution initialization time {"seconds"=>0.73}
[2023-03-15T07:49:37.859+0000] {subprocess.py:93} INFO - [2023-03-15T07:49:37,858][INFO ][logstash.javapipeline    ][main] Pipeline started {"pipeline.id"=>"main"}
[2023-03-15T07:49:37.958+0000] {subprocess.py:93} INFO - [2023-03-15T07:49:37,958][INFO ][logstash.agent           ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]}
[2023-03-15T07:49:38.953+0000] {subprocess.py:93} INFO - [2023-03-15T07:49:38,953][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.014505s) SELECT CAST(current_setting('server_version_num') AS integer) AS v
[2023-03-15T07:49:39.071+0000] {subprocess.py:93} INFO - [2023-03-15T07:49:39,071][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.021744s) SELECT count(*) AS "count" FROM (SELECT * FROM customers) AS "t1" LIMIT 1
[2023-03-15T07:49:39.259+0000] {subprocess.py:93} INFO - [2023-03-15T07:49:39,259][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.171797s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 0
[2023-03-15T07:49:43.894+0000] {subprocess.py:93} INFO - [2023-03-15T07:49:43,894][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.215044s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 50000
[2023-03-15T07:49:47.363+0000] {subprocess.py:93} INFO - [2023-03-15T07:49:47,362][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.239696s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 100000
[2023-03-15T07:49:49.360+0000] {subprocess.py:93} INFO - [2023-03-15T07:49:49,359][INFO ][logstash.inputs.jdbc     ][main][afa454a497099ce3438919c7cfc7ae10a3b3980082608321cda443e0b5f70dbd] (0.078431s) SELECT * FROM (SELECT * FROM customers) AS "t1" LIMIT 50000 OFFSET 150000
[2023-03-15T07:49:50.916+0000] {subprocess.py:93} INFO - [2023-03-15T07:49:50,915][INFO ][logstash.javapipeline    ][main] Pipeline terminated {"pipeline.id"=>"main"}
[2023-03-15T07:49:51.025+0000] {subprocess.py:93} INFO - [2023-03-15T07:49:51,025][INFO ][logstash.pipelinesregistry] Removed pipeline from registry successfully {:pipeline_id=>:main}
[2023-03-15T07:49:51.051+0000] {subprocess.py:93} INFO - [2023-03-15T07:49:51,051][INFO ][logstash.runner          ] Logstash shut down.
[2023-03-15T07:49:51.130+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-03-15T07:49:51.147+0000] {taskinstance.py:1318} INFO - Marking task as SUCCESS. dag_id=AUTOMATISATION, task_id=Chargement_des_données_in_ELK, execution_date=20230313T000533, start_date=20230315T074915, end_date=20230315T074951
[2023-03-15T07:49:51.180+0000] {local_task_job.py:208} INFO - Task exited with return code 0
[2023-03-15T07:49:51.186+0000] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
